<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Sauradip Nag</title>

  <meta name="author" content="Sauradip Nag">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="stylesheet" type="text/css" href="style.css">
  <link rel="icon" type="image/jpeg" href="images/icon.jpeg">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Sauradip Nag</name>
                    <br>
                    <font size="3"><br />Teaching machines to <br />THINK LIKE HUMANS.</font>
                  </p>
                  <br />
                  <p align="justify">
                    <font size="3">
                      I am a Project Associate in <a href="http://www.cse.iitm.ac.in/~vplab/" target="_blank">
                        <font size="3">Visualization and Percepion Lab </font>
                      </a>, IIT Madras working on a DRDO Project which involves finding out hidden location from a
                      unknown environment.
                    </font>
                  </p>
                  <p align="justify">
                    <font size="3">
                      I did my Bachelor's in Computer Science and Engineering from Kalyani Government Engineering
                      College (KGEC), Kalyani, India.
                      During my undergrad, I got the opportunity to work with <a
                        href="https://www.isical.ac.in/~umapada/" target="_blank">
                        <font size="3">Prof. Umapada Pal</font>
                      </a>, Head of <a href="https://www.isical.ac.in/~cvpr/" target="_blank">
                        <font size="3">Computer Vision & Pattern Recognition Unit</font>
                      </a> at <a href="https://www.isical.ac.in/" target="_blank">
                        <font size="3">Indian Statistical Institute (ISI), Kolkata</font>
                      </a> and <a href="https://umexpert.um.edu.my/shiva" target="_blank">
                        <font size="3">Prof. Palaihnakhote Shivakumara</font>
                      </a> of <a href="https://www.um.edu.my/" target="_blank">
                        <font size="3">University of Malaya , Malyasia</font>
                      </a> on various Computer Vision research problems. I also did my research project under <a
                        href="https://www.iitr.ac.in/departments/CSE/pages/People+Faculty+Partha_Pratim_Roy.html"
                        target="_blank">
                        <font size="3">Prof. Partha Pratim Roy</font>
                      </a> of <a href="https://www.iitr.ac.in/" target="_blank">
                        <font size="3">Indian Institute of Technolgy (IIT), Roorkee</font>
                      </a> where I worked on Facial Microexpression. I completed my bachelor's thesis on <i>Interacting
                        with Softwares using Gestures</i> under the guidance of Prof. Koushik Dasgupta of KGEC and Mr.
                      Tamojit Chatterjee of Indeed Inc.
                    </font>
                  </p>
                  <br>
                  <p style="text-align:center">
                    <a href="mailto:sauradipnag95@gmail.com" target="_blank">
                      <font size="3">Email</font>
                    </a> &nbsp/&nbsp
                    <a href="Sauradip_Nag_CV.pdf" target="_blank">CV</a> &nbsp/&nbsp
                    <!-- <a href="data/JonBarron-bio.txt">Biography</a> &nbsp/&nbsp -->
                    <a href="https://scholar.google.com/citations?user=hlkMCO4AAAAJ&hl=en" target="_blank">
                      <font size="3">Google Scholar</font>
                    </a> &nbsp/&nbsp
                    <a href="https://www.linkedin.com/in/sauradip-nag-1b6059112/" target="_blank">
                      <font size="3"> LinkedIn </font>
                    </a> &nbsp/&nbsp
                    <a href="https://github.com/sauradip" target="_blank">
                      <font size="3"> GitHub </font>
                    </a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/profile.jpg" target="_blank"><img style="width:100%;max-width:100%"
                      alt="Profile Photo" src="images/circle-cropped.png" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- RESEARCH -->

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle;text-align:justify">
                  <heading>
                    <font size="5">Research Interests</font>
                  </heading>
                  <p>
                    <font size="3">
                      I am broadly interested in the field of Computer Vision and Deep Learning. Particularly, I like to
                      think upon <b>Visual Scene Understanding (VSU)</b> from images and videos, effective methods of
                      <b>Domain Adaptation & Transfer Learning</b> for VSU, building <b>systems that learn with minimal
                        or no supervision</b> and <b>systems that generalize well</b> in real and diverse scenarios.
                      I am also open to any topic that would be interesting or fun to explore and pursue.
                    </font>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>


                                            <!-- Updates on Recent Activities  -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle;text-align:justify">
                  <heading>
                    <font size="5">Updates</font>
                  </heading>
                  <p>
                    <font size="3">
                      <!-- <a class="button" href="#" style="color:#FFA500"><strong style="font-size:12px">New</strong></a> -->
                      <!-- <button type="submit" class="button">Click me!</button> -->
                      <ul style="padding-inline-start:0px;list-style-type:none;">
                        <li> <a class="button" href="#" style="color:#FFA500"><strong style="font-size:12px">New</strong></a><em style="font-size:16px;"> 1 paper on Future Frame Depth Prediction communicated to <strong style="font-size:16px">ECCV 2020</strong></em></li>
                      <p style="margin-block-start:0px;margin-block-end:3px"></p>
                        <li> <a class="button" href="#" style="color:#FFA500"><strong style="font-size:12px">New</strong></a> <em style="font-size:16px;"> I am joining <strong style="font-size:16px"> SketchX Lab, University of Surrey</strong> as a PhD student in September 2020</em></li>
                      <p style="margin-block-start:0px;margin-block-end:3px"></p>
                        <li> <a class="button" href="#" style="color:#FFA500"><strong style="font-size:12px">New</strong></a> <em style="font-size:16px;"> Our paper titled "What's There in The Dark" has been selected in <strong style="font-size:16px">TOP 10% Papers</strong> in <strong style="font-size:16px">ICIP19</strong></em></li>
                      <p style="margin-block-start:0px;margin-block-end:3px"></p>
                        <li style="padding-left:38px;"> <em style="font-size:16px;"> 1 paper on <strong style="font-size:16px">Light-weight Saliency Detection</strong> is uploaded on Arxiv</em></li>
                      </ul>
                    </font>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>




          <!-- PUBLICATIONS -->

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>
                    <font size="5">Publications</font>
                  </heading>
                </td>
              </tr>
              <tr>
                <td style="padding:20px;padding-bottom: 0px;width:100%;vertical-align:middle">
                  <heading>
                    <font size="4">Journals</font>
                  </heading>
                </td>
              </tr>
            </tbody>
          </table>

          <script>
            function myFunction(pub_name) {
              var x = document.getElementById(pub_name);
              if (x.style.display === 'none') {
                x.style.display = 'block';
              } else {
                x.style.display = 'none';
              }
            }
          </script>





          <!-- JOURNAL SECTION        -->

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <!-- UNDER REVIEW - BIB NUMBER DETECTION -->
              <tr onmouseout="underreview_0_stop()" onmouseover="underreview_0_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='underreview_0_image'><img src='images/pr_icon(1).png'></div>
                    <img src='images/pr_icon(1).png'>
                  </div>
                  <script type="text/javascript">
                    function underreview_0_start() {
                      document.getElementById('underreview_0_image').style.opacity = "1";
                    }

                    function underreview_0_stop() {
                      document.getElementById('underreview_0_image').style.opacity = "0";
                    }
                    underreview_0_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="TODO" target="_blank">
                    <papertitle>
                      <font size="3">A New Unified Method for Text Detection in Marathon and Sports Video</font>
                    </papertitle>
                  </a>
                  <br>
                  <font size="3">
                    <strong>
                      <font size="3">Sauradip Nag</font>
                    </strong>,
                    <a href="https://sudip.info/" target="_blank">
                      <font size="3">P Shivakumara</font>
                    </a>,
                    <a href="https://www.isical.ac.in/~ujjwal/" target="_blank">
                      <font size="3">Umapada Pal</font>
                    </a>,
                    <a href="https://www.isical.ac.in/~ujjwal/" target="_blank">
                      <font size="3">Tong Lu</font>
                    </a>,
                    <a href="https://www.isical.ac.in/~ujjwal/" target="_blank">
                      <font size="3">Michael Blumenstein</font>
                    </a>
                    <br>
                    <em>Under Review in <strong>
                        <font size="3" style="color:#105c24">Pattern Recognition, Elsevier (IF:5.98)</font>
                      </strong></em>, 2019
                  </font>
                  <br>
                  <p></p>
                </td>
              </tr>
            </tbody>
          </table>




          <!-- CONFERENCE SECTION -->


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>
                    <font size="4">Conference</font>
                  </heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <!-- UNDER REVIEW - POSE ESTIMATION -->
              <tr onmouseout="icip_stop()" onmouseover="icip_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='icip_image'><img src='images/seg_output_re.png'></div>
                    <img src='images/night_input_re.png'>
                  </div>
                  <script type="text/javascript">
                    function icip_start() {
                      document.getElementById('icip_image').style.opacity = "1";
                    }

                    function icip_stop() {
                      document.getElementById('icip_image').style.opacity = "0";
                    }
                    icip_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="TODO" target="_blank">
                    <papertitle>
                      <font size="3">What's There in the Dark
                      </font>
                    </papertitle>
                  </a>
                  <br>
                  <font size="3">
                    <strong>
                      <font size="3">Sauradip Nag</font>
                    </strong>,
                    <a href="https://sudip.info/" target="_blank">
                      <font size="3">Saptakatha Adak</font>
                    </a>,
                    <a href="https://www.isical.ac.in/~ujjwal/" target="_blank">
                      <font size="3">Sukhendu Das</font>
                    </a>
                    <br>
                    <em>International Conference in Image Processing (ICIP)</em>, 2019
                    <font color="red" size="3"><strong>(Spotlight Paper)</strong></font>
                    <br>
                    <em>Taipei, Taiwan </em>
                    <strong>
                      <font size="3" style="color:#FF8C00">[ H5-Index : 45 ]</font>
                    </strong>
                  </font>
                  <br>
                  <p></p>
                  <a href="javascript:void(0);" onclick="myFunction('icip_abs')">
                    <font size="3">Abstract</font>
                  </a> /
                  <a href="https://github.com/sauradip/night_image_semantic_segmentation" target="_blank">
                    <font size="3">Code</font>
                  </a> /
                  <!-- <a href="https://arxiv.org/abs/1811.01401" target="_blank"><font size="3">arXiv</font></a> / -->
                  <a href="javascript:void(0);" onclick="myFunction('icip_bib')">
                    <font size="3">BibTex</font>
                  </a>
                  <p></p>
                  <div id="icip_abs" style="display:none; text-align:justify;min-width:350px;">
                    <font size="3">
                      <em>
                        Scene Parsing is an important cog for modern autonomous driving systems. Most of the works in
                        semantic segmentation pertains to day-time scenes with favourable weather and illumination
                        conditions. In this paper, we propose a novel deep architecture, NiSeNet, that performs semantic
                        segmentation of night scenes using a domain mapping approach of synthetic to real data. It is a
                        dual-channel network, where we designed a Real channel using DeepLabV3+ coupled with an MSE loss
                        to preserve the spatial information. In addition, we used an Adaptive channel reducing the
                        domain gap between synthetic and real night images, which also complements the failures of Real
                        channel output. Apart from the dual channel, we introduced a novel fusion scheme to fuse the
                        outputs of two channels. In addition to that, we compiled a new dataset Urban Night Driving
                        Dataset (UNDD); it consists of 7125 unlabelled day and night images; additionally, it has 75
                        night images with pixel-level annotations having classes equivalent to Cityscapes dataset. We
                        evaluated our approach on the Berkley Deep Drive dataset, the challenging Mapillary dataset and
                        UNDD dataset to exhibit that the proposed method outperforms the state-of-the-art techniques in
                        terms of accuracy and visual quality.
                      </em>
                    </font>
                  </div>
                  <div id="icip_bib" style="font-family:Courier;display:none;min-width:350px;">
                    <font size="2">
                      <br>
                      <!-- @article{kishore2019cluenet,<br>
                      title={ClueNet: A Deep Framework for Occluded Pedestrian Pose Estimation},<br>
                      author={Kishore, Perla Sai Raj and Das, Sudip and Mukherjee, Partha Sarathi and Bhattacharya,
                      Ujjwal},<br>
                      year={2019}<br>
                      } -->
                      @inproceedings{nag2019s,<br>
                      title={What’s There in the Dark},<br>
                      author={Nag, Sauradip and Adak, Saptakatha and Das, Sukhendu},<br>
                      booktitle={2019 IEEE International Conference on Image Processing (ICIP)},<br>
                      pages={2996--3000},<br>
                      year={2019},<br>
                      organization={IEEE}<br>
                      }
                    </font>
                  </div>
                </td>
              </tr>







              <!-- BMVC 2019 - POSE ESTIMATION -->
              <tr onmouseout="icassp_stop()" onmouseover="icassp_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='icassp_image'><img src='images/icassp_2_re.png'></div>
                    <img src='images/icassp_re.png'>
                  </div>
                  <script type="text/javascript">
                    function icassp_start() {
                      document.getElementById('icassp_image').style.opacity = "1";
                    }

                    function icassp_stop() {
                      document.getElementById('icassp_image').style.opacity = "0";
                    }
                    icassp_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/abstract/document/8683737" target="_blank">
                    <papertitle>
                      <font size="3">Facial Micro-Expression Spotting and Recognition using Time Contrasted Feature with
                        Visual Memory</font>
                    </papertitle>
                  </a>
                  <br>
                  <font size="3">
                    <strong>
                      <font size="3">Sauradip Nag</font>
                    </strong>,
                    <a href="https://sudip.info/" target="_blank">
                      <font size="3">Ayan Kumar Bhunia</font>
                    </a>,
                    Aishik Konwer,
                    <a href="https://www.isical.ac.in/~ujjwal/" target="_blank">
                      <font size="3">Partha Pratim Roy</font>
                    </a>
                    <br>
                    <em>International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2019
                        <br>
                        <em>Brighton, United Kingdom </em>
                        <strong>
                        <font size="3" style="color:#FF8C00">[ H5-Index : 80 ]</font>
                      </strong>

                  </font>
                  <br>
                  <p></p>
                  <a href="javascript:void(0);" onclick="myFunction('icassp_abs')">
                    <font size="3">Abstract</font>
                  </a> /
                  <!-- <a href="https://arxiv.org/abs/1811.01401" target="_blank"><font size="3">arXiv</font></a> / -->
                  <a href="javascript:void(0);" onclick="myFunction('icassp_bib')">
                    <font size="3">BibTex</font>
                  </a>
                  <p></p>
                  <div id="icassp_abs" style="display:none; text-align:justify;min-width:350px;">
                    <font size="3">
                      <em>
                        Facial micro-expressions are sudden involuntary minute muscle movements which reveal true
                        emotions that people try to conceal. Spotting a micro-expression and recognizing it is a major
                        challenge owing to its short duration and intensity. Many works pursued traditional and deep
                        learning based approaches to solve this issue but compromised on learning low level features and
                        higher accuracy due to unavailability of datasets. This motivated us to propose a novel joint
                        architecture of spatial and temporal network which extracts time-contrasted features from the
                        feature maps to contrast out micro-expression from rapid muscle movements. The usage of time
                        contrasted features greatly improved the spotting of micro-expression from inconspicuous facial
                        movements. Also, we include a memory module to predict the class and intensity of the
                        micro-expression across the temporal frames of the micro-expression clip. Our method achieves
                        superior performance in comparison to other conventional approaches on CASMEII dataset.
                      </em>
                    </font>
                  </div>
                  <div id="icassp_bib" style="font-family:Courier;display:none;min-width:350px;">
                    <font size="2">
                      <br>
                      @inproceedings{nag2019facial,<br>
                        title={Facial Micro-Expression Spotting and Recognition using Time Contrasted Feature with Visual Memory},<br>
                        author={Nag, Sauradip and Bhunia, Ayan Kumar and Konwer, Aishik and Roy, Partha Pratim},<br>
                        booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},<br>
                        pages={2022--2026},<br>
                        year={2019},<br>
                        organization={IEEE}<br>
                      }<br>
                    </font>
                  </div>
                </td>
              </tr>


              <!-- CVPR 2019 - HANDWRITING RECOGNITION -->
              <tr onmouseout="icdar_stop()" onmouseover="icdar_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='icdar_image'><img src='images/icdar_2_re.png'></div>
                    <img src='images/icdar_re.png'>
                  </div>
                  <script type="text/javascript">
                    function icdar_start() {
                      document.getElementById('icdar_image').style.opacity = "1";
                    }

                    function icdar_stop() {
                      document.getElementById('icdar_image').style.opacity = "0";
                    }
                    icdar_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Bhunia_Handwriting_Recognition_in_Low-Resource_Scripts_Using_Adversarial_Learning_CVPR_2019_paper.html"
                    target="_blank">
                    <papertitle>
                      <font size="3">CRNN based Jersey-Bib Number/Text Recognition in Sports and Marathon Images</font>
                    </papertitle>
                  </a>
                  <br>

                  <font size="3">
                    <strong>
                      <font size="3">Sauradip Nag</font>
                    </strong>,
                    <a href="https://www.surrey.ac.uk/people/ayan-kumar-bhunia" target="_blank">
                      <font size="3">Raghavendra Ramachandra</font>
                    </a>,
                    <a href="https://ankanbhunia.github.io/" target="_blank">
                      <font size="3">Palaiahnakote Shivakumara</font>
                    </a>,
                    <a href="https://www.iitr.ac.in/departments/CSE/pages/People+Faculty+Partha_Pratim_Roy.html"
                      target="_blank">
                      <font size="3">Umapada Pal</font>
                    </a>,
                    <a href="https://www.iitr.ac.in/departments/CSE/pages/People+Faculty+Partha_Pratim_Roy.html"
                      target="_blank">
                      <font size="3">Tong Lu</font>
                    </a>,
                    <a href="https://www.iitr.ac.in/departments/CSE/pages/People+Faculty+Partha_Pratim_Roy.html"
                      target="_blank">
                      <font size="3">Mohan Kankanhalli</font>
                    </a>
                    <br>
                    <em>International Conference on Document Analysis and Recognition (ICDAR)</em>, 2019
                      <br>
                      <em>Sydney, Australia </em>
                      <strong>
                        <font size="3" style="color:#FF8C00">[ H5-Index : 26 ]</font></strong>
                  </font>
                  <br>
                  <p></p>
                  <a href="javascript:void(0);" onclick="myFunction('icdar_abs')">
                    <font size="3">Abstract</font>
                  </a> /
                  <!-- <a href="https://github.com/AyanKumarBhunia/Handwriting_Recogition_using_Adversarial_Learning"
                    target="_blank">
                    <font size="3">Code</font>
                  </a> / -->
                  <!-- <a href="https://arxiv.org/abs/1811.01396" target="_blank" target="_blank">
                    <font size="3">arXiv</font>
                  </a> / -->
                  <a href="javascript:void(0);" onclick="myFunction('icdar_bib')">
                    <font size="3">BibTex</font>
                  </a>
                  <p></p>
                  <div id="icdar_abs" style="display:none; text-align:justify;min-width:350px;">
                    <font size="3">
                      <em>
                        The primary challenge in tracing the participants in sports and marathon video or images is to detect and localize the jersey/Bib number that may present in different regions of their outfit captured in cluttered environment conditions. In this work, we proposed a new framework based on detecting the human body parts such that both Jersey Bib number and text is localized reliably. To achieve this, the proposed method first detects and localize the human in a given image using Single Shot Multibox Detector (SSD). In the next step, different human body parts namely, Torso, Left Thigh, Right Thigh, that generally contain a Bib number or text region is automatically extracted. These detected individual parts are processed individually to detect the Jersey Bib number/text using a deep CNN network based on the 2-channel architecture based on the novel adaptive weighting loss function. Finally, the detected text is cropped out and fed to a CNN-RNN based deep model abbreviated as CRNN for recognizing jersey/Bib/text. Extensive experiments are carried out on the four different datasets including both bench-marking dataset and a new dataset. The performance of the proposed method is compared with the state-of-the-art methods on all four datasets that indicates the improved performance of the proposed method on all four datasets.
                      </em>
                    </font>
                  </div>
                  <div id="icdar_bib" style="font-family:Courier;display:none;min-width:350px;">
                    <font size="2">
                      <br>
                      @inproceedings{nag2019crnn,<br>
                      title={CRNN Based Jersey-Bib Number/Text Recognition in Sports and Marathon Images},<br>
                      author={Nag, Sauradip and Ramachandra, Raghavendra and Shivakumara, Palaiahnakote and Pal, Umapada and Lu, Tong and Kankanhalli, Mohan},<br>
                      booktitle={2019 International Conference on Document Analysis and Recognition (ICDAR)},<br>
                      pages={1149--1156},<br>
                      year={2019},<br>
                      organization={IEEE}<br>
                    }
                    </font>
                  </div>
                </td>
              </tr>


              <!-- ICASSP 2019 - THUMBNAIL GENERATION -->
              <tr onmouseout="icfhr_stop()" onmouseover="icfhr_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='icfhr_image'><img src='images/icfhr_re_2.png'></div>
                    <img src='images/icfhr_re.png'>
                  </div>
                  <script type="text/javascript">
                    function icfhr_start() {
                      document.getElementById('icfhr_image').style.opacity = "1";
                    }

                    function icfhr_stop() {
                      document.getElementById('icfhr_image').style.opacity = "0";
                    }
                    icfhr_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/document/8683761" target="_blank">
                    <papertitle>
                      <font size="3">A New COLD Feature based Handwriting Analysis for Ethnicity/Nationality Identification</font>
                    </papertitle>
                  </a>
                  <br>
                  <font size="3">
                    <strong>
                      <font size="3">Sauradip Nag</font>
                    </strong>,
                    <a href="https://www.surrey.ac.uk/people/ayan-kumar-bhunia" target="_blank">
                      <font size="3">Palaiahnakote Shivakumara</font>
                    </a>,
                    <a href="https://www.surrey.ac.uk/people/ayan-kumar-bhunia" target="_blank">
                      <font size="3">Wu Yirui</font>
                    </a>,
                    <a href="https://www.surrey.ac.uk/people/ayan-kumar-bhunia" target="_blank">
                      <font size="3">Umapada Pal</font>
                    </a>,

                    <a href="https://www.iitr.ac.in/departments/CSE/pages/People+Faculty+Partha_Pratim_Roy.html"
                      target="_blank">
                      <font size="3">Tong Lu</font>
                    </a>
                    <br>
                    <em>International Conference on Frontiers in Handwriting Recognition (ICFHR)</em>, 2018
                      <br>
                      <em>Niagara Falls, USA </em>
                      <strong>
                        <font size="3" style="color:#FF8C00">[ H5-Index : 18 ]</font></strong>
                    <!-- <br> -->
                    <!-- <font color="red" size="3"><strong>(Oral Presentation)</strong></font> -->
                  </font>
                  <br>
                  <p></p>
                  <a href="javascript:void(0);" onclick="myFunction('icfhr_abs')">
                    <font size="3">Abstract</font>
                  </a> /
                  <!-- <a href="https://github.com/sairajk/Thumbnail-Generation" target="_blank">
                    <font size="3">Code</font>
                  </a> /
                  <a href="https://arxiv.org/abs/1810.13054" target="_blank">
                    <font size="3">arXiv</font>
                  </a> / -->
                  <a href="javascript:void(0);" onclick="myFunction('icfhr_bib')">
                    <font size="3">BibTex</font>
                  </a>
                  <p></p>
                  <div id="icfhr_abs" style="display:none; text-align:justify;min-width:350px;">
                    <font size="3">
                      <em>
                        Identifying crime for forensic investigating teams
                        when crimes involve people of different nationals is challenging.
                        This paper proposes a new method for ethnicity (nationality)
                        identification based on Cloud of Line Distribution (COLD)
                        features of handwriting components. The proposed method, at
                        first, explores tangent angle for the contour pixels in each row
                        and the mean of intensity values of each row in an image for
                        segmenting text lines. For segmented text lines, we use tangent
                        angle and direction of base lines to remove rule lines in the
                        image. We use polygonal approximation for finding dominant
                        points for contours of edge components. Then the proposed
                        method connects the nearest dominant points of every dominant
                        point, which results in line segments of dominant point pairs. For
                        each line segment, the proposed method estimates angle and
                        length, which gives a point in polar domain. For all the line
                        segments, the proposed method generates dense points in polar
                        domain, which results in COLD distribution. As character
                        component shapes change, according to nationals, the shape of
                        the distribution changes. This observation is extracted based on
                        distance from pixels of distribution to Principal Axis of the
                        distribution. Then the features are subjected to an SVM classifier
                        for identifying nationals. Experiments are conducted on a
                        complex dataset, which show the proposed method is effective
                        and outperforms the existing method.
                      </em>
                    </font>
                  </div>
                  <div id="icfhr_bib" style="font-family:Courier;display:none;min-width:350px;">
                    <font size="2">
                      <br>
                      @inproceedings{nag2018new,<br>
                      title={New COLD Feature Based Handwriting Analysis for Enthnicity/Nationality Identification},<br>
                      author={Nag, Sauradip and Shivakumara, Palaiahnakote and Wu, Yirui and Pal, Umapada and Lu, Tong},<br>
                      booktitle={2018 16th International Conference on Frontiers in Handwriting Recognition (ICFHR)},<br>
                      pages={523--527},<br>
                      year={2018},<br>
                      organization={IEEE}<br>
                    }
                    </font>
                  </div>
                </td>
              </tr>


              <!-- WACV 2019 - TEXTURE PAPER -->
              <tr onmouseout="cicba_stop()" onmouseover="cicba_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='cicba_image'><img src='images/cicba_2_re.png'></div>
                    <img src='images/cicba_re.png'>
                  </div>
                  <script type="text/javascript">
                    function cicba_start() {
                      document.getElementById('cicba_image').style.opacity = "1";
                    }

                    function cicba_stop() {
                      document.getElementById('cicba_image').style.opacity = "0";
                    }
                    cicba_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/document/8659343" target="_blank">
                    <papertitle>
                      <font size="3">Offline Extraction of Indic Regional Language from Natural Scene Image Using Text Segmentation and Deep Convolutional Sequence</font>
                    </papertitle>
                  </a>
                  <br>
                  <font size="3">
                    <strong>
                      <font size="3">Sauradip Nag</font>
                    </strong>,
                    <a href="https://www.surrey.ac.uk/people/ayan-kumar-bhunia" target="_blank">
                      <font size="3">Pallab Ganguly</font>
                    </a>,
                    <a href="https://www.linkedin.com/in/pranay-mukherjee-701588137/" target="_blank">
                      <font size="3">Sumit Roy</font>
                    </a>,
                    <a href="https://www.linkedin.com/in/pranay-mukherjee-701588137/" target="_blank">
                      <font size="3">Sourab Jha</font>
                    </a>,
                    <a href="https://www.linkedin.com/in/pranay-mukherjee-701588137/" target="_blank">
                      <font size="3"> Krishna Bose</font>
                    </a>,
                    <a href="https://www.linkedin.com/in/pranay-mukherjee-701588137/" target="_blank">
                      <font size="3"> Abhishek Jha</font>
                    </a>,
                    Abhirup Das,
                    <a href="https://www.iitr.ac.in/departments/CSE/pages/People+Faculty+Partha_Pratim_Roy.html"
                      target="_blank">
                      <font size="3"> Kousik Dasgupta</font>
                    </a>
                    <br>
                    <em>International Conference on Computational Intelligence, Communications, and Business Analytics (CICBA)</em>, 2018
                    <br>
                    <em> Kalyani, India <em>
                  </font>
                  <br>
                  <p></p>
                  <a href="javascript:void(0);" onclick="myFunction('cicba_abs')">
                    <font size="3">Abstract</font>
                  </a> /
                  <a href="https://arxiv.org/abs/1811.01401" target="_blank">
                    <font size="3">arXiv</font>
                  </a> /
                  <a href="javascript:void(0);" onclick="myFunction('cicba_bib')">
                    <font size="3">BibTex</font>
                  </a>
                  <p></p>
                  <div id="cicba_abs" style="display:none; text-align:justify;min-width:350px;">
                    <font size="3">
                      <em>
                        With the large scale explosion of images and videos over the internet, efficient hashing methods
                        have been developed to facilitate memory and time efficient retrieval of similar images.
                        However, none of the existing works use hashing to address texture image retrieval mostly
                        because of the lack of sufficiently large texture image databases. Our work addresses this
                        problem by developing a novel deep learning architecture that generates binary hash codes for
                        input texture images. For this, we first pre-train a Texture Synthesis Network (TSN) which takes
                        a texture patch as input and outputs an enlarged view of the texture by injecting newer texture
                        content. Thus it signifies that the TSN encodes the learnt texture specific information in its
                        intermediate layers. In the next stage, a second network gathers the multi-scale feature
                        representations from the TSN’s intermediate layers using channel-wise attention, combines them
                        in a progressive manner to a dense continuous representation which is finally converted into a
                        binary hash code with the help of individual and pairwise label information. The new enlarged
                        texture patches from the TSN also help in data augmentation to alleviate the problem of
                        insufficient texture data and are used to train the second stage of the network. Experiments on
                        three public texture image retrieval datasets indicate the superiority of our texture synthesis
                        guided hashing approach over existing state-of-the-art methods.
                      </em>
                    </font>
                  </div>
                  <div id="cicba_bib" style="font-family:Courier;display:none;min-width:350px;">
                    <font size="2">
                      <br>
                      @inproceedings{bhunia2019texture,<br>
                      title={Texture synthesis guided deep hashing for texture image retrieval},<br>
                      author={Bhunia, Ayan Kumar and Perla, Sai Raj Kishore and Mukherjee, Pranay and Das, Abhirup and
                      Roy, Partha Pratim},<br>
                      booktitle={2019 IEEE Winter Conference on Applications of Computer Vision (WACV)},<br>
                      pages={609--618},<br>
                      year={2019},<br>
                      organization={IEEE}<br>
                      }
                    </font>
                  </div>
                </td>
              </tr>


              <!-- <tr onmouseout="loss_stop()" onmouseover="loss_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='loss_image'><img src='images/loss_after.png'></div>
                <img src='images/loss_before.png'>
              </div>
              <script type="text/javascript">
                function loss_start() {
                  document.getElementById('loss_image').style.opacity = "1";
                }

                function loss_stop() {
                  document.getElementById('loss_image').style.opacity = "0";
                }
                loss_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=1xpZ0fL9h1y9RfcTyPgVkxUrF3VwdkBvq">
                <papertitle>A General and Adaptive Robust Loss Function</papertitle>
              </a>
              <br>
              <strong>Jonathan T. Barron</strong>
              <br>
              <em>CVPR</em>, 2019 &nbsp <font color="red"><strong>(Oral Presentation, Best Paper Award Finalist)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/1701.03077">arxiv</a> /
              <a href="https://drive.google.com/open?id=1HNveL7xSNh6Ss7sxLK8Mw2L1Fc-rRhL4">supplement</a> /
              <a href="https://youtu.be/BmNKbnF69eY">video</a> /
              <a href="https://www.youtube.com/watch?v=4IInDT_S0ow&t=37m22s">talk</a> /
              <a href="https://drive.google.com/file/d/1GzRYRIfLHvNLT_QwjHoBjHkBbs3Nbf0x/view?usp=sharing">slides</a> /
              <a href="https://github.com/google-research/google-research/tree/master/robust_loss">tensorflow code</a> /
              <a href="https://github.com/jonbarron/robust_loss_pytorch">pytorch code</a> /
              <a href="data/BarronCVPR2019_reviews.txt">reviews</a> /
              <a href="data/BarronCVPR2019.bib">bibtex</a>
              <p></p>
              <p>A single robust loss function is a superset of many other common robust loss functions, and allows training to automatically adapt the robustness of its own loss.</p>
            </td>
          </tr> -->

            </tbody>
          </table>

          <!-- Projects -->

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <br><br>
                  <heading>
                    <font size="5">Projects</font>
                  </heading>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>


              <!-- PROJECT - SALIENCY DETECTION, CVPR19, PyTorch -->
              <tr onmouseout="proj_sod_stop()" onmouseover="proj_sod_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='proj_sod_image'><img src='images/Proj-SOD_after_re.png'></div>
                    <img src='images/Proj-SOD_before_re.png'>
                  </div>
                  <script type="text/javascript">
                    function proj_sod_start() {
                      document.getElementById('proj_sod_image').style.opacity = "1";
                    }

                    function proj_sod_stop() {
                      document.getElementById('proj_sod_image').style.opacity = "0";
                    }
                    proj_sod_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle;text-align:justify">
                  <a href="https://github.com/sairajk/PyTorch-Pyramid-Feature-Attention-Network-for-Saliency-Detection"
                    target="_blank">
                    <papertitle>
                      <font size="3">Saliency Detection: PyTorch implementation of a CVPR 2019 Publication</font>
                    </papertitle>
                  </a>
                  <br><br>
                  <font size="3">
                    PyTorch implementation of the paper "<i>Pyramid Feature Attention Network for Saliency
                      Detection</i>", published at CVPR 2019.
                  </font>
                  <br>
                  <p></p>
                  <a href="https://github.com/sairajk/PyTorch-Pyramid-Feature-Attention-Network-for-Saliency-Detection"
                    target="_blank">
                    <font size="3">Code</font>
                  </a> /
                  <a href="https://arxiv.org/abs/1903.00179" target="_blank">
                    <font size="3">Paper</font>
                  </a>
                  <p></p>
                </td>
              </tr>


              <!-- PROJECT - IMAGE SUPER-RESOLUTION APPLICATION -->
              <tr onmouseout="proj_sisr_stop()" onmouseover="proj_sisr_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='proj_sisr_image'><img src='images/Proj-sisr_after_re.jpg'></div>
                    <img src='images/Proj-sisr_before_re.png'>
                  </div>
                  <script type="text/javascript">
                    function proj_sisr_start() {
                      document.getElementById('proj_sisr_image').style.opacity = "1";
                    }

                    function proj_sisr_stop() {
                      document.getElementById('proj_sisr_image').style.opacity = "0";
                    }
                    proj_sisr_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle;text-align:justify">
                  <a href="https://github.com/sairajk/Image-Super-Resolution-Application" target="_blank">
                    <papertitle>
                      <font size="3">Single Image Super Resolution</font>
                    </papertitle>
                  </a>
                  <br><br>
                  <font size="3">
                    <!-- Image super resolution is the process of increasing the resolution of a Low Resolution (LR) image by generating pixels which interpolate best between the given LR image and the required High Resolution (HR) image. The code was written using Keras (TF backend) in Python3 and comes with an easy to use interface. -->
                    <!-- The task has numerous applications like in Satellite Imaging, Surveillance, Medical image processing, Biometrics, etc. -->
                    Image super resolution aims to increase the resolution of an image by generating pixels which
                    interpolate best between a given Low Resolution and the required High Resolution image. I built a
                    deep learning based model for this purpose. A large amount of diverse data was also collected to
                    train this model. The model was implemented in Keras and comes with an easy to use interface. This
                    was my project as an intern under <a href="https://www.iiitd.ac.in/subramanyam" target="_blank">
                      <font size="3">Prof. A. V. Subramanyam</font>
                    </a> of <a href="https://www.iiitd.ac.in" target="_blank">
                      <font size="3">IIIT, Delhi</font>
                    </a>.
                  </font>
                  <br>
                  <p></p>
                  <a href="https://github.com/sairajk/Image-Super-Resolution-Application" target="_blank">
                    <font size="3">Code</font>
                  </a> /
                  <a href="https://github.com/YapengTian/Single-Image-Super-Resolution" target="_blank">
                    <font size="3">Papers on Super Resolution</font>
                  </a>
                  <p></p>
                </td>
              </tr>


              <!-- PROJECT - MIXTURE DENSITY NETWORKS -->
              <tr onmouseout="proj_mdn_stop()" onmouseover="proj_mdn_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='proj_mdn_image'><img src='images/Proj-MDN_re.png'></div>
                    <img src='images/Proj-MDN_re.png'>
                  </div>
                  <script type="text/javascript">
                    function proj_mdn_start() {
                      document.getElementById('proj_mdn_image').style.opacity = "1";
                    }

                    function proj_mdn_stop() {
                      document.getElementById('proj_mdn_image').style.opacity = "0";
                    }
                    proj_mdn_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle;text-align:justify">
                  <a href="https://github.com/sairajk/Mixture-Density-Networks" target="_blank">
                    <papertitle>
                      <font size="3">Mixture Density Networks</font>
                    </papertitle>
                  </a>
                  <br><br>
                  <font size="3">
                    Mixture Density Networks (MDNs) are an interesting way to address multimodality (where the input and
                    output hold a one-to-many relationship). In such scenarios, instead of directly predicting the
                    output we model the probability distribution of the output as a weighed mixture of several Gaussians
                    from which we sample the actual output.
                    <!-- Usually, the output distribution is a weighed mixture of several similar distributions and hence the name "Mixture Density Networks". -->
                    In this project, I implemented univariate and bivariate MDNs in Python using Tensorflow.
                  </font>
                  <br>
                  <p></p>
                  <a href="https://github.com/sairajk/Mixture-Density-Networks" target="_blank">
                    <font size="3">Code</font>
                  </a> /
                  <!-- <a href="http://blog.otoro.net/2015/11/24/mixture-density-networks-with-tensorflow/" target="_blank"><font size="3">Interesting Blog</font></a> / -->
                  <a href="https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf" target="_blank">
                    <font size="3">Original Paper</font>
                  </a>
                  <p></p>
                </td>
              </tr>


              <!-- PROJECT - LANGUAGE MODEL -->
              <tr onmouseout="proj_lm_stop()" onmouseover="proj_lm_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='proj_lm_image'><img src='images/Proj-lm_after_re.png'></div>
                    <img src='images/Proj-lm_before_re.png'>
                  </div>
                  <script type="text/javascript">
                    function proj_lm_start() {
                      document.getElementById('proj_lm_image').style.opacity = "1";
                    }

                    function proj_lm_stop() {
                      document.getElementById('proj_lm_image').style.opacity = "0";
                    }
                    proj_lm_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle;text-align:justify">
                  <a href="https://github.com/sairajk/Language-Models" target="_blank">
                    <papertitle>
                      <font size="3">Character Level Language Model</font>
                    </papertitle>
                  </a>
                  <br><br>
                  <font size="3">
                    Auto-correct and auto-complete, which have now become a standard feature in almost all digital
                    keyboards, make use of a language model at its core. In this project, I built a LSTM based
                    character-level language model that aims to predict the next character from a sequence of input
                    characters. The code for this project was written in Python using Tensorflow.
                  </font>
                  <br>
                  <p></p>
                  <a href="https://github.com/sairajk/Language-Models" target="_blank">
                    <font size="3">Code</font>
                  </a>
                  <p></p>
                </td>
              </tr>


              <!-- PROJECT - LANE DETECTION NFS U2 -->
              <tr onmouseout="proj_lanedet_stop()" onmouseover="proj_lanedet_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='proj_lanedet_image'><img src='images/Proj-Lane_Det_re.png'></div>
                    <img src='images/Proj-Lane_Det_re.png'>
                  </div>
                  <script type="text/javascript">
                    function proj_lanedet_start() {
                      document.getElementById('proj_lanedet_image').style.opacity = "1";
                    }

                    function proj_lanedet_stop() {
                      document.getElementById('proj_lanedet_image').style.opacity = "0";
                    }
                    proj_lanedet_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle;text-align:justify">
                  <a href="https://github.com/sairajk/Lane-Detection-in-NFS-Underground-2" target="_blank">
                    <papertitle>
                      <font size="3">Lane Detection in <i>NFS: Underground 2</i></font>
                    </papertitle>
                  </a>
                  <br><br>
                  <font size="3">
                    Self Driving cars are one of the fascinating technologies in this modern world. Though the entire
                    process, from perceiving the surroundings to getting the car to move, is fairly complex, the first
                    step usually begins with detection of lanes which guide the vehicle on the road. In this project, I
                    attempt to do the same in one the popular games "<i>NFS: Underground 2</i>" using OpenCV in Python.
                  </font>
                  <br>
                  <p></p>
                  <a href="https://github.com/sairajk/Lane-Detection-in-NFS-Underground-2" target="_blank">
                    <font size="3">Code</font>
                  </a>
                  <p></p>
                </td>
              </tr>


              <!-- PROJECT - ML ALGORITHMS -->
              <tr onmouseout="proj_mlalgo_stop()" onmouseover="proj_mlalgo_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='proj_mlalgo_image'><img src='images/Proj-MlAlgo_after_re.jpg'></div>
                    <img src='images/Proj-MlAlgo_before_re.png'>
                  </div>
                  <script type="text/javascript">
                    function proj_mlalgo_start() {
                      document.getElementById('proj_mlalgo_image').style.opacity = "1";
                    }

                    function proj_mlalgo_stop() {
                      document.getElementById('proj_mlalgo_image').style.opacity = "0";
                    }
                    proj_mlalgo_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle;text-align:justify">
                  <a href="https://github.com/sairajk/Machine-Learning-Algorithms" target="_blank">
                    <papertitle>
                      <font size="3">Machine Learning Algorithms</font>
                    </papertitle>
                  </a>
                  <br><br>
                  <font size="3">
                    In this project, I implemented various Machine Learning algorithms from scratch in Python using only
                    Numpy.
                  </font>
                  <br>
                  <p></p>
                  <a href="https://github.com/sairajk/Machine-Learning-Algorithms" target="_blank">
                    <font size="3">Code</font>
                  </a>
                  <p></p>
                </td>
              </tr>

            </tbody>
          </table>

          <!-- Footer - Template Credits -->

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:center;font-size:small;">
                    Template credits :
                    <a href="https://jonbarron.info/" target="_blank">Dr. Jon Barron</a>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>


        </td>
      </tr>
  </table>
</body>

</html>
