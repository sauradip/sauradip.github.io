<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Sauradip Nag</title>

  <meta name="author" content="Sauradip Nag">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="stylesheet" type="text/css" href="style.css">
  <link rel="icon" type="image/jpeg" href="images/icon.jpeg">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Sauradip Nag</name>
                    <br>
                    <font size="3"><br />Teaching machines to <br />THINK LIKE HUMANS.</font>
                  </p>
                  <br />
                  <p align="justify">
                    <font size="3">
                      I am a Doctor of Philosophy (PhD) student, focusing on Computer Vision and Deep Learning, at
                      <a
                        href="http://sketchx.eecs.qmul.ac.uk/" target="_blank">
                        <font size="3">SketchX Lab.</font></a> of Centre for Vision, Speech and Signal Processing (CVSSP), University of Surrey,
                      England, United Kingdom. My primary supervisor is <a
                        href="http://personal.ee.surrey.ac.uk/Personal/T.Xiang/index.html" target="_blank">
                      <font size="3">Prof. Tao(Tony) Xiang</font></a>, and co-supervisors are
                      <a
                        href="http://personal.ee.surrey.ac.uk/Personal/Y.Song/index.html" target="_blank">
                        <font size="3">Prof. Yi-Zhe Song</font></a> and <a
                        href="https://yang.ac/" target="_blank">
                      <font size="3"> Prof. Yongxin Yang</font></a>. I also work closely with <a
                        href="https://xiatian-zhu.github.io/" target="_blank">
                      <font size="3">Eddy</font></a> from <a href="https://research.samsung.com/aicenter_cambridge" target="_blank"> <font size="3">Samsung Research, UK</font></a>. 
                    </font>
                  </p>
                  <p align="justify">
                    <font size="3">
                      Prior to this, I was a Project Associate in <a href="http://www.cse.iitm.ac.in/~vplab/"
                        target="_blank">
                        <font size="3">Visualization and Percepion Lab </font>
                      </a>, IIT Madras working on a DRDO Project under <a
                        href="http://www.cse.iitm.ac.in/~sdas/" target="_blank">
                      <font size="3">Prof. Sukhendu Das</font></a> which involves finding out hidden location from a
                      unknown environment.
                    </font>
                  </p>
                  <p align="justify">
                    <font size="3">
                      I did my Bachelor's in Computer Science and Engineering from Kalyani Government Engineering
                      College (KGEC), Kalyani, India.
                      During my undergrad, I got the opportunity to work with <a
                        href="https://www.isical.ac.in/~umapada/" target="_blank">
                        <font size="3">Prof. Umapada Pal</font>
                      </a>, Head of <a href="https://www.isical.ac.in/~cvpr/" target="_blank">
                        <font size="3">Computer Vision & Pattern Recognition Unit</font>
                      </a> at <a href="https://www.isical.ac.in/" target="_blank">
                        <font size="3">Indian Statistical Institute (ISI), Kolkata</font>
                      </a> and <a href="https://umexpert.um.edu.my/shiva" target="_blank">
                        <font size="3">Prof. Palaihnakhote Shivakumara</font>
                      </a> of <a href="https://www.um.edu.my/" target="_blank">
                        <font size="3">University of Malaya , Malyasia</font>
                      </a> on various Computer Vision research problems. I also did my research project under <a
                        href="https://www.iitr.ac.in/departments/CSE/pages/People+Faculty+Partha_Pratim_Roy.html"
                        target="_blank">
                        <font size="3">Prof. Partha Pratim Roy</font>
                      </a> of <a href="https://www.iitr.ac.in/" target="_blank">
                        <font size="3">Indian Institute of Technolgy (IIT), Roorkee</font>
                      </a> where I worked on Facial Microexpression. I completed my bachelor's thesis on <i>Interacting
                        with Softwares using Gestures</i> under the guidance of Prof. Koushik Dasgupta of KGEC and Mr.
                      Tamojit Chatterjee of Indeed Inc.
                    </font>
                    <br>
                    <br>
                    <font size="4" color="blue" style="font-weight:bold">Office :</font> <font color="grey" size="3">You can primarily find me sitting in Desk 3, Room 01BB01, CVSSP ( Behind AP Plaza ), University of Surrey, UK. Will be happy to meet and talk over some cup of coffee beside the majestic Lake ! </font>
                  </p>
                  <br>
                  <p style="text-align:center">
                    <a href="mailto:sauradipnag95@gmail.com" target="_blank">
                      <font size="3">Email</font>
                    </a> &nbsp/&nbsp
                    <a href="Sauradip_Nag_CV.pdf" target="_blank">CV</a> &nbsp/&nbsp
                    <!-- <a href="data/JonBarron-bio.txt">Biography</a> &nbsp/&nbsp -->
                    <a href="https://scholar.google.com/citations?user=hlkMCO4AAAAJ&hl=en" target="_blank">
                      <font size="3">Google Scholar</font>
                    </a> &nbsp/&nbsp
                    <a href="https://www.linkedin.com/in/sauradip-nag-1b6059112/" target="_blank">
                      <font size="3"> LinkedIn </font>
                    </a> &nbsp/&nbsp
                    <a href="https://github.com/sauradip" target="_blank">
                      <font size="3"> GitHub </font>
                    </a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/profile.jpg" target="_blank"><img style="width:100%;max-width:100%"
                      alt="Profile Photo" src="images/rsz_saura6_crop.png" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- RESEARCH -->

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle;text-align:justify">
                  <heading>
                    <font size="5">Research Interests</font>
                  </heading>
                  <p>
                    <font size="3">
                      I am broadly interested in the field of Computer Vision and Deep Learning. Particularly, I like to
                      think upon <b>Visual Scene Understanding (VSU)</b> from images and videos, effective methods of
                      <b>Domain Adaptation & Transfer Learning</b> for VSU, building <b>systems that learn with minimal
                        or no supervision</b> and <b>systems that generalize well</b> in real and diverse scenarios.
                      I am also open to any topic that would be interesting or fun to explore and pursue.
                    </font>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>


          <!-- Updates on Recent Activities  -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td
                  style="padding:20px;padding-top:5px;padding-bottom:5px;width:100%;vertical-align:middle;text-align:justify">
                  <heading>
                    <font size="5">Updates</font>
                  </heading>
                  <p>
                    <font size="3">
                      <!-- <a class="button" href="#" style="color:#FFA500"><strong style="font-size:12px">New</strong></a> -->
                      <!-- <button type="submit" class="button">Click me!</button> -->
                      <ul style="padding-inline-start:0px;list-style-type:none;">
                        <!--                         <li> <a class="button" href="#" style="color:#FFA500"><strong style="font-size:12px">New</strong></a><em style="font-size:16px;"> 1 paper on Future Frame Depth Prediction communicated to <strong style="font-size:16px">ECCV 2020</strong></em></li> -->
                        <p style="margin-block-start:0px;margin-block-end:3px"></p>
                        <li> <a class="button" href="#" style="color:#FFA500"><strong
                              style="font-size:12px">New</strong></a> <strong
                            style="font-size:15px;padding-left:5px;">09/2020 : </strong>
                          <em style="font-size:16px;"> I have moved to South-West London to join<strong style="font-size:16px"> SketchX
                              Lab,
                              University of Surrey</strong> as a PhD student.



                        <p style="margin-block-start:0px;margin-block-end:3px"></p>
                        <li> <a class="button" href="#" style="color:#FFA500"><strong
                              style="font-size:12px">New</strong></a> <strong
                            style="font-size:15px;padding-left:5px;">05/2020 : </strong>
                          <em style="font-size:16px;"> <strong>1</strong> paper communicated to <strong
                              style="font-size:16px"> <a
                                href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76"
                                style="font-size:16px">IEEE Transactions on Circuits and Systems for Video
                                Technology</a> </strong></em></li>

                        <p style="margin-block-start:0px;margin-block-end:3px"></p>
                        <li> <a class="button" href="#" style="color:#FFA500"><strong
                              style="font-size:12px">New</strong></a> <strong
                            style="font-size:15px;padding-left:5px;">05/2020 : </strong>
                          <em style="font-size:16px;"> <strong>1</strong> paper got accepted in <strong
                              style="font-size:16px"> <a href="https://www.journals.elsevier.com/pattern-recognition"
                                style="font-size:16px">Pattern Recognition, Elsevier</a> </strong></em></li>


                        <p style="margin-block-start:0px;margin-block-end:3px"></p>
                        <li style="padding-left:40px;">
                          <!-- <a class="button" href="#" style="color:#FFA500"><strong
                              style="font-size:12px">New</strong></a>  -->
                          <strong style="font-size:15px;padding-left:5px;">11/2019 : </strong>
                          <em style="font-size:16px;"> Our paper titled
                            "What's There in The Dark" has been selected in <strong style="font-size:16px"><a
                                style="font-size:16px" href="http://2019.ieeeicip.org/?action=page3&id=9">TOP 10%
                                Papers</a></strong> in <strong style="font-size:16px">ICIP19</strong></em></li>

                        <p style="margin-block-start:0px;margin-block-end:3px"></p>
                        <li style="padding-left:45px;"> <em style="font-size:16px;"></em> <strong
                            style="font-size:15px">10/2019 : </strong> </em> <strong>1</strong> paper on <strong
                            style="font-size:16px">Light-weight Saliency Detection</strong> is uploaded on Arxiv</em>
                        </li>
                      </ul>
                    </font>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <br>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td
                  style="padding:20px;padding-top:5px;padding-bottom:5px;width:100%;vertical-align:middle;text-align:justify">
                  <heading>
                    <font size="5">Research Experience</font>
                  </heading>

                </td>
              </tr>
            </tbody>
          </table>

          <table style="padding:20px;padding-bottom:0px" border="0" cellpadding="0" cellspacing="4">

            <tr>
              <td valign="center" rowspan=6><img height=75 width=90 border=0 src="images/UOS_1.jpg">

            <tr>
              <td>
              <td>
              <td>
              <td>
              <td>
              <td valign="top" colspan="2"><span style="font-size: 16px;" class="h1"></b><b>University of Surrey,
                    UK</b></span>

                <br><em>Under Prof. Tao Xiang and Prof. Yi-Zhe Song</em><br>
                Jul 2020 - Present
            </tr>
      </tr>
  </table>

  <table style="padding:20px;" border="0" cellpadding="0" cellspacing="4">

    <tr>
      <td valign="center" rowspan=6><img height=90 border=0 src="images/re_iitm.png">

    <tr>
      <td>
      <td>
      <td>
      <td>
      <td>
      <td valign="top" colspan="2"><span style="font-size: 16px;" class="h1"></b><b>Indian Institute of
            Technology Madras,
            India</b></span>

        <br><em>Under Prof. Sukhendu Das</em><br>
        Jul 2018 - Jul 2020
    </tr>
    </tr>
  </table>
  <!-- <p></p> -->

  <table style="padding:20px;padding-top:0px;" border="0" cellpadding="0" cellspacing="4">

    <tr>
      <td valign="center" rowspan=6><img height=90 border=0 src="images/re_isi_logo.png">

    <tr>
      <td>
      <td>
      <td>
      <td>
      <td>
      <td valign="top" colspan="2"><span style="font-size: 16px;" class="h1"></b><b>Indian Statistical Institute
            Kolkata, India</b></span>

        <br><em>Under Dr. Umapada Pal</em><br>
        Jul 2016 - Jul 2018
  </table>

  <br>
  <!-- PUBLICATIONS -->

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:20px;padding-top:5px;padding-bottom:3px;width:100%;vertical-align:middle">
          <heading>
            <font size="5">Publications</font>
          </heading>
        </td>
      </tr>
      <tr>
        <td
          style="padding:20px;padding-top:5px;padding-bottom:5px;padding-bottom: 0px;width:100%;vertical-align:middle">
          <heading>
            <font size="4">Journals</font>
          </heading>
        </td>
      </tr>
    </tbody>
  </table>

  <script>
    function myFunction(pub_name) {
      var x = document.getElementById(pub_name);
      if (x.style.display === 'none') {
        x.style.display = 'block';
      } else {
        x.style.display = 'none';
      }
    }
  </script>





  <!-- JOURNAL SECTION        -->

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <!-- UNDER REVIEW - BIB NUMBER DETECTION -->
      <tr onmouseout="underreview_0_stop()" onmouseover="underreview_0_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='underreview_0_image'><img src='images/pr_after_re.png'></div>
            <img src='images/pr_icon(1).png'>
          </div>
          <script type="text/javascript">
            function underreview_0_start() {
              document.getElementById('underreview_0_image').style.opacity = "1";
            }

            function underreview_0_stop() {
              document.getElementById('underreview_0_image').style.opacity = "0";
            }
            underreview_0_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://www.sciencedirect.com/science/article/abs/pii/S003132032030279X" target="_blank">
            <papertitle>
              <font size="3">A New Unified Method for Detecting Text from Marathon Runners and Sports Players in
                Video</font>
            </papertitle>
          </a>
          <br>
          <p></p>
          <font size="3">
            <strong>
              <font size="3">Sauradip Nag</font>
            </strong>,
            <a href="https://sudip.info/" target="_blank">
              <font size="3">P Shivakumara</font>
            </a>,
            <a href="https://www.isical.ac.in/~ujjwal/" target="_blank">
              <font size="3">Umapada Pal</font>
            </a>,
            <a href="https://www.isical.ac.in/~ujjwal/" target="_blank">
              <font size="3">Tong Lu</font>
            </a>,
            <a href="https://www.isical.ac.in/~ujjwal/" target="_blank">
              <font size="3">Michael Blumenstein</font>
            </a>
            <br>
            <em><strong>
                <font size="3" style="color:#105c24">Pattern Recognition, Elsevier</font>
                <font size="3" style="color:#0911eb"> [IF : 7.196]</font>
              </strong></em>
          </font>
          <br>
          <p></p>
          <a href="javascript:void(0);" onclick="myFunction('pr_abs')">
            <font size="3">Abstract</font>
          </a> /
          <a href="https://github.com/sauradip/PR_scene_text" target="_blank">
            <font size="3">Code</font>
          </a> /
          <!-- <a href="https://arxiv.org/abs/1811.01401" target="_blank"><font size="3">arXiv</font></a> / -->
          <a href="javascript:void(0);" onclick="myFunction('pr_bib')">
            <font size="3">BibTex</font>
          </a>
          <p></p>
          <div id="pr_abs" style="display:none; text-align:justify;min-width:350px;">
            <font size="3">
              <em>
                Detecting text located on the torsos of marathon runners and sports players in video is a
                challenging
                issue due to poor quality and adverse effects caused by flexible/colorful clothing, and
                different
                structures of human bodies or actions. This paper presents a new unified method for tackling the
                above
                challenges. The proposed method fuses gradient magnitude and direction coherence of text pixels
                in a
                new way for detecting candidate regions. Candidate regions are used for determining the number
                of
                temporal frame clusters obtained by K-means clustering on frame differences. This process in
                turn
                detects key frames. The proposed method explores Bayesian probability for skin portions using
                color
                values at both pixel and component levels of temporal frames, which provides fused images with
                skin
                components. Based on skin information, the proposed method then detects faces and torsos by
                finding
                structural and spatial coherences between them. We further propose adaptive pixels linking a
                deep
                learning model for text detection from torso regions. The proposed method is tested on our own
                dataset
                collected from marathon/sports video and three standard datasets, namely, RBNR, MMM and R-ID of
                marathon images, to evaluate the performance. In addition, the proposed method is also tested on
                the
                standard natural scene datasets, namely, CTW1500 and MS-COCO text datasets, to show the
                objectiveness
                of the proposed method. A comparative study with the state-of-the-art methods on bib number/text
                detection of different datasets shows that the proposed method outperforms the existing methods
              </em>
            </font>
          </div>
          <div id="pr_bib" style="font-family:Courier;display:none;min-width:350px;">
            <font size="2">
              <br>
              <!-- @article{kishore2019cluenet,<br>
                      title={ClueNet: A Deep Framework for Occluded Pedestrian Pose Estimation},<br>
                      author={Kishore, Perla Sai Raj and Das, Sudip and Mukherjee, Partha Sarathi and Bhattacharya,
                      Ujjwal},<br>
                      year={2019}<br>
                      } -->
              @article{nag2020new,
              title={A New Unified Method for Detecting Text from Marathon Runners and Sports Players in Video
              (PR-D-19-01078R2)},
              author={Nag, Sauradip and Shivakumara, Palaiahnakote and Pal, Umapada and Lu, Tong and Blumenstein,
              Michael},
              journal={Pattern Recognition},
              pages={107476},
              year={2020},
              publisher={Elsevier}
              }
            </font>
          </div>

        </td>
      </tr>
    </tbody>
  </table>

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <!-- UNDER REVIEW - BIB NUMBER DETECTION -->
      <tr onmouseout="underreview_1_stop()" onmouseover="underreview_1_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='underreview_1_image'><img src='images/re_archi_tip.png'></div>
            <img src='images/re_tip_1.jpg'>
          </div>
          <script type="text/javascript">
            function underreview_1_start() {
              document.getElementById('underreview_1_image').style.opacity = "1";
            }

            function underreview_1_stop() {
              document.getElementById('underreview_1_image').style.opacity = "0";
            }
            underreview_1_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="TODO" target="_blank">
            <papertitle>
              <font size="3">Deep Network for Text Detection on Uniform/Jersey in Sports Images</font>
            </papertitle>
          </a>
          <br>
          <p></p>
          <font size="3">
            <a href="https://www.isical.ac.in/~ujjwal/" target="_blank">
              <font size="3">P Chowdhury</font>
            </a>,
            <a href="https://sudip.info/" target="_blank">
              <font size="3">P Shivakumara</font>
            </a>,
            <a href="https://www.isical.ac.in/~ujjwal/" target="_blank">
              <font size="3">R Ramachandra</font>
            </a>,
            <strong>
              <font size="3">Sauradip Nag</font>
            </strong>,
            <a href="https://www.isical.ac.in/~ujjwal/" target="_blank">
              <font size="3">Umapada Pal</font>
            </a>,
            <a href="https://www.isical.ac.in/~ujjwal/" target="_blank">
              <font size="3">Tong Lu</font>
            </a>,
            <a href="https://www.isical.ac.in/~ujjwal/" target="_blank">
              <font size="3">Daniel Lopresti</font>
            </a>
            <br>
            <em>Communicated to <strong>
                <font size="3" style="color:#105c24">IEEE Transactions on CSVT</font>
                <font size="3" style="color:#0911eb"> [IF : 4.05]</font>
              </strong></em>
          </font>
          <br>
          <p></p>
          <a href="javascript:void(0);" onclick="myFunction('tip_abs')">
            <font size="3">Abstract</font>
          </a>
          <!-- <a href="https://github.com/sauradip/PR_scene_text" target="_blank">
            <font size="3">Code</font>
          </a> / -->
          <!-- <a href="https://arxiv.org/abs/1811.01401" target="_blank"><font size="3">arXiv</font></a> / -->
          <!-- <a href="javascript:void(0);" onclick="myFunction('pr_bib')">
            <font size="3">BibTex</font>
          </a> -->
          <p></p>
          <div id="tip_abs" style="display:none; text-align:justify;min-width:350px;">
            <font size="3">
              <em>
                Due to explosive proliferation of multimedia content especially from sports images, retrieving desired
                information using common devices is an interesting research area. It is also challenging because of low
                image quality, large variations of camera view-points, large pose variations, occlusions etc. This paper
                presents a new method for text detection on jersey/clothing in sports images. Unlike most existing
                methods which explore torso, face and skin for detecting texts in sports images, we explore
                cloth/uniform/jersey information for text detection. The basis for exploiting cloth, which can be jersey
                or uniform of the player, is the presence of vital text in the cloth in sports images. The proposed
                method integrates Residual Network (ResNet) and Pyramidal Pooling Module (PPM) for generating a spatial
                attention map over cloth region and the Progressive Scalable Expansion Algorithm (PSE) for text
                detection from cloth regions. Experimental results on our dataset that contains sports images and the
                benchmark datasets, namely, RBNR, MMM which provide Marathon images, Re-ID which is a person
                re-identification dataset, show that the proposed method outperforms the existing methods in terms of
                precision and F1-score. In addition, the proposed approach is also tested on sports images chosen from
                benchmark datasets of natural scene text detection like CTW1500 and MS-COCO and we noted that the
                proposed method is robust, effective and reliable.
              </em>
            </font>
          </div>
          <div id="pr_bib" style="font-family:Courier;display:none;min-width:350px;">
            <font size="2">
              <br>
              <!-- @article{kishore2019cluenet,<br>
                      title={ClueNet: A Deep Framework for Occluded Pedestrian Pose Estimation},<br>
                      author={Kishore, Perla Sai Raj and Das, Sudip and Mukherjee, Partha Sarathi and Bhattacharya,
                      Ujjwal},<br>
                      year={2019}<br>
                      } -->
              @inproceedings{nag2019s,<br>
              title={What’s There in the Dark},<br>
              author={Nag, Sauradip and Adak, Saptakatha and Das, Sukhendu},<br>
              booktitle={2019 IEEE International Conference on Image Processing (ICIP)},<br>
              pages={2996--3000},<br>
              year={2019},<br>
              organization={IEEE}<br>
              }
            </font>
          </div>

        </td>
      </tr>
    </tbody>
  </table>




  <!-- CONFERENCE SECTION -->


  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:20px;padding-top:3px;padding-bottom:5px;width:100%;vertical-align:middle">
          <heading>
            <font size="4">Conference</font>
          </heading>
        </td>
      </tr>
    </tbody>
  </table>
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>

      <!-- UNDER REVIEW - POSE ESTIMATION -->
      <tr onmouseout="icip_stop()" onmouseover="icip_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='icip_image'><img src='images/seg_output_re.png'></div>
            <img src='images/night_input_re.png'>
          </div>
          <script type="text/javascript">
            function icip_start() {
              document.getElementById('icip_image').style.opacity = "1";
            }

            function icip_stop() {
              document.getElementById('icip_image').style.opacity = "0";
            }
            icip_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://ieeexplore.ieee.org/abstract/document/8803299" target="_blank">
            <papertitle>
              <font size="3">What's There in the Dark
              </font>
            </papertitle>
          </a>
          <br>
          <p></p>
          <font size="3">
            <strong>
              <font size="3">Sauradip Nag</font>
            </strong>,
            <a href="https://sudip.info/" target="_blank">
              <font size="3">Saptakatha Adak</font>
            </a>,
            <a href="https://www.isical.ac.in/~ujjwal/" target="_blank">
              <font size="3">Sukhendu Das</font>
            </a>
            <br>
            <em>International Conference in Image Processing (ICIP)</em>, 2019
            <font color="red" size="3"><strong>(Spotlight Paper)</strong></font>
            <br>
            <em>Taipei, Taiwan </em>
            <strong>
              <font size="3" style="color:#FF8C00">[ H5-Index : 45 ]</font>
            </strong>
          </font>
          <br>
          <p></p>
          <a href="javascript:void(0);" onclick="myFunction('icip_abs')">
            <font size="3">Abstract</font>
          </a> /
          <a href="https://github.com/sauradip/night_image_semantic_segmentation" target="_blank">
            <font size="3">Code</font>
          </a> /
          <!-- <a href="https://arxiv.org/abs/1811.01401" target="_blank"><font size="3">arXiv</font></a> / -->
          <a href="javascript:void(0);" onclick="myFunction('icip_bib')">
            <font size="3">BibTex</font>
          </a>
          <p></p>
          <div id="icip_abs" style="display:none; text-align:justify;min-width:350px;">
            <font size="3">
              <em>
                Scene Parsing is an important cog for modern autonomous driving systems. Most of the works in
                semantic segmentation pertains to day-time scenes with favourable weather and illumination
                conditions. In this paper, we propose a novel deep architecture, NiSeNet, that performs semantic
                segmentation of night scenes using a domain mapping approach of synthetic to real data. It is a
                dual-channel network, where we designed a Real channel using DeepLabV3+ coupled with an MSE loss
                to preserve the spatial information. In addition, we used an Adaptive channel reducing the
                domain gap between synthetic and real night images, which also complements the failures of Real
                channel output. Apart from the dual channel, we introduced a novel fusion scheme to fuse the
                outputs of two channels. In addition to that, we compiled a new dataset Urban Night Driving
                Dataset (UNDD); it consists of 7125 unlabelled day and night images; additionally, it has 75
                night images with pixel-level annotations having classes equivalent to Cityscapes dataset. We
                evaluated our approach on the Berkley Deep Drive dataset, the challenging Mapillary dataset and
                UNDD dataset to exhibit that the proposed method outperforms the state-of-the-art techniques in
                terms of accuracy and visual quality.
              </em>
            </font>
          </div>
          <div id="icip_bib" style="font-family:Courier;display:none;min-width:350px;">
            <font size="2">
              <br>
              <!-- @article{kishore2019cluenet,<br>
                      title={ClueNet: A Deep Framework for Occluded Pedestrian Pose Estimation},<br>
                      author={Kishore, Perla Sai Raj and Das, Sudip and Mukherjee, Partha Sarathi and Bhattacharya,
                      Ujjwal},<br>
                      year={2019}<br>
                      } -->
              @inproceedings{nag2019s,<br>
              title={What’s There in the Dark},<br>
              author={Nag, Sauradip and Adak, Saptakatha and Das, Sukhendu},<br>
              booktitle={2019 IEEE International Conference on Image Processing (ICIP)},<br>
              pages={2996--3000},<br>
              year={2019},<br>
              organization={IEEE}<br>
              }
            </font>
          </div>
        </td>
      </tr>







      <!-- BMVC 2019 - POSE ESTIMATION -->
      <tr onmouseout="icassp_stop()" onmouseover="icassp_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='icassp_image'><img src='images/icassp_2_re.png'></div>
            <img src='images/icassp_re.png'>
          </div>
          <script type="text/javascript">
            function icassp_start() {
              document.getElementById('icassp_image').style.opacity = "1";
            }

            function icassp_stop() {
              document.getElementById('icassp_image').style.opacity = "0";
            }
            icassp_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://ieeexplore.ieee.org/abstract/document/8683737" target="_blank">
            <papertitle>
              <font size="3">Facial Micro-Expression Spotting and Recognition using Time Contrasted Feature with
                Visual Memory</font>
            </papertitle>
          </a>
          <br>
          <p></p>
          <font size="3">
            <strong>
              <font size="3">Sauradip Nag</font>
            </strong>,
            <a href="https://sudip.info/" target="_blank">
              <font size="3">Ayan Kumar Bhunia</font>
            </a>,
            Aishik Konwer,
            <a href="https://www.isical.ac.in/~ujjwal/" target="_blank">
              <font size="3">Partha Pratim Roy</font>
            </a>
            <br>
            <em>International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2019
            <br>
            <em>Brighton, United Kingdom </em>
            <strong>
              <font size="3" style="color:#FF8C00">[ H5-Index : 80 ]</font>
            </strong>

          </font>
          <br>
          <p></p>
          <a href="javascript:void(0);" onclick="myFunction('icassp_abs')">
            <font size="3">Abstract</font>
          </a> /
          <a href="https://arxiv.org/abs/1902.03514" target="_blank">
            <font size="3">arXiv</font>
          </a> /
          <a href="javascript:void(0);" onclick="myFunction('icassp_bib')">
            <font size="3">BibTex</font>
          </a>
          <p></p>
          <div id="icassp_abs" style="display:none; text-align:justify;min-width:350px;">
            <font size="3">
              <em>
                Facial micro-expressions are sudden involuntary minute muscle movements which reveal true
                emotions that people try to conceal. Spotting a micro-expression and recognizing it is a major
                challenge owing to its short duration and intensity. Many works pursued traditional and deep
                learning based approaches to solve this issue but compromised on learning low level features and
                higher accuracy due to unavailability of datasets. This motivated us to propose a novel joint
                architecture of spatial and temporal network which extracts time-contrasted features from the
                feature maps to contrast out micro-expression from rapid muscle movements. The usage of time
                contrasted features greatly improved the spotting of micro-expression from inconspicuous facial
                movements. Also, we include a memory module to predict the class and intensity of the
                micro-expression across the temporal frames of the micro-expression clip. Our method achieves
                superior performance in comparison to other conventional approaches on CASMEII dataset.
              </em>
            </font>
          </div>
          <div id="icassp_bib" style="font-family:Courier;display:none;min-width:350px;">
            <font size="2">
              <br>
              @inproceedings{nag2019facial,<br>
              title={Facial Micro-Expression Spotting and Recognition using Time Contrasted Feature with Visual
              Memory},<br>
              author={Nag, Sauradip and Bhunia, Ayan Kumar and Konwer, Aishik and Roy, Partha Pratim},<br>
              booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal
              Processing (ICASSP)},<br>
              pages={2022--2026},<br>
              year={2019},<br>
              organization={IEEE}<br>
              }<br>
            </font>
          </div>
        </td>
      </tr>


      <!-- CVPR 2019 - HANDWRITING RECOGNITION -->
      <tr onmouseout="icdar_stop()" onmouseover="icdar_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='icdar_image'><img src='images/icdar_2_re.png'></div>
            <img src='images/icdar_re.png'>
          </div>
          <script type="text/javascript">
            function icdar_start() {
              document.getElementById('icdar_image').style.opacity = "1";
            }

            function icdar_stop() {
              document.getElementById('icdar_image').style.opacity = "0";
            }
            icdar_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://ieeexplore.ieee.org/abstract/document/8977968" target="_blank">
            <papertitle>
              <font size="3">CRNN based Jersey-Bib Number/Text Recognition in Sports and Marathon Images</font>
            </papertitle>
          </a>
          <br>
          <p></p>
          <font size="3">
            <strong>
              <font size="3">Sauradip Nag</font>
            </strong>,
            <a href="https://www.surrey.ac.uk/people/ayan-kumar-bhunia" target="_blank">
              <font size="3">Raghavendra Ramachandra</font>
            </a>,
            <a href="https://ankanbhunia.github.io/" target="_blank">
              <font size="3">Palaiahnakote Shivakumara</font>
            </a>,
            <a href="https://www.iitr.ac.in/departments/CSE/pages/People+Faculty+Partha_Pratim_Roy.html"
              target="_blank">
              <font size="3">Umapada Pal</font>
            </a>,
            <a href="https://www.iitr.ac.in/departments/CSE/pages/People+Faculty+Partha_Pratim_Roy.html"
              target="_blank">
              <font size="3">Tong Lu</font>
            </a>,
            <a href="https://www.iitr.ac.in/departments/CSE/pages/People+Faculty+Partha_Pratim_Roy.html"
              target="_blank">
              <font size="3">Mohan Kankanhalli</font>
            </a>
            <br>
            <em>International Conference on Document Analysis and Recognition (ICDAR)</em>, 2019
            <br>
            <em>Sydney, Australia </em>
            <strong>
              <font size="3" style="color:#FF8C00">[ H5-Index : 26 ]</font>
            </strong>
          </font>
          <br>
          <p></p>
          <a href="javascript:void(0);" onclick="myFunction('icdar_abs')">
            <font size="3">Abstract</font>
          </a> /
          <!-- <a href="https://github.com/AyanKumarBhunia/Handwriting_Recogition_using_Adversarial_Learning"
                    target="_blank">
                    <font size="3">Code</font>
                  </a> / -->
          <!-- <a href="https://arxiv.org/abs/1811.01396" target="_blank" target="_blank">
                    <font size="3">arXiv</font>
                  </a> / -->
          <a href="javascript:void(0);" onclick="myFunction('icdar_bib')">
            <font size="3">BibTex</font>
          </a>
          <p></p>
          <div id="icdar_abs" style="display:none; text-align:justify;min-width:350px;">
            <font size="3">
              <em>
                The primary challenge in tracing the participants in sports and marathon video or images is to
                detect and localize the jersey/Bib number that may present in different regions of their outfit
                captured in cluttered environment conditions. In this work, we proposed a new framework based on
                detecting the human body parts such that both Jersey Bib number and text is localized reliably.
                To achieve this, the proposed method first detects and localize the human in a given image using
                Single Shot Multibox Detector (SSD). In the next step, different human body parts namely, Torso,
                Left Thigh, Right Thigh, that generally contain a Bib number or text region is automatically
                extracted. These detected individual parts are processed individually to detect the Jersey Bib
                number/text using a deep CNN network based on the 2-channel architecture based on the novel
                adaptive weighting loss function. Finally, the detected text is cropped out and fed to a CNN-RNN
                based deep model abbreviated as CRNN for recognizing jersey/Bib/text. Extensive experiments are
                carried out on the four different datasets including both bench-marking dataset and a new
                dataset. The performance of the proposed method is compared with the state-of-the-art methods on
                all four datasets that indicates the improved performance of the proposed method on all four
                datasets.
              </em>
            </font>
          </div>
          <div id="icdar_bib" style="font-family:Courier;display:none;min-width:350px;">
            <font size="2">
              <br>
              @inproceedings{nag2019crnn,<br>
              title={CRNN Based Jersey-Bib Number/Text Recognition in Sports and Marathon Images},<br>
              author={Nag, Sauradip and Ramachandra, Raghavendra and Shivakumara, Palaiahnakote and Pal, Umapada
              and Lu, Tong and Kankanhalli, Mohan},<br>
              booktitle={2019 International Conference on Document Analysis and Recognition (ICDAR)},<br>
              pages={1149--1156},<br>
              year={2019},<br>
              organization={IEEE}<br>
              }
            </font>
          </div>
        </td>
      </tr>


      <!-- ICASSP 2019 - THUMBNAIL GENERATION -->
      <tr onmouseout="icfhr_stop()" onmouseover="icfhr_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='icfhr_image'><img src='images/icfhr_re_2.png'></div>
            <img src='images/icfhr_re.png'>
          </div>
          <script type="text/javascript">
            function icfhr_start() {
              document.getElementById('icfhr_image').style.opacity = "1";
            }

            function icfhr_stop() {
              document.getElementById('icfhr_image').style.opacity = "0";
            }
            icfhr_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://ieeexplore.ieee.org/abstract/document/8583815/" target="_blank">
            <papertitle>
              <font size="3">A New COLD Feature based Handwriting Analysis for Ethnicity/Nationality
                Identification</font>
            </papertitle>
          </a>
          <br>
          <p></p>
          <font size="3">
            <strong>
              <font size="3">Sauradip Nag</font>
            </strong>,
            <a href="https://www.surrey.ac.uk/people/ayan-kumar-bhunia" target="_blank">
              <font size="3">Palaiahnakote Shivakumara</font>
            </a>,
            <a href="https://www.surrey.ac.uk/people/ayan-kumar-bhunia" target="_blank">
              <font size="3">Wu Yirui</font>
            </a>,
            <a href="https://www.surrey.ac.uk/people/ayan-kumar-bhunia" target="_blank">
              <font size="3">Umapada Pal</font>
            </a>,

            <a href="https://www.iitr.ac.in/departments/CSE/pages/People+Faculty+Partha_Pratim_Roy.html"
              target="_blank">
              <font size="3">Tong Lu</font>
            </a>
            <br>
            <em>International Conference on Frontiers in Handwriting Recognition (ICFHR)</em>, 2018
            <br>
            <em>Niagara Falls, USA </em>
            <strong>
              <font size="3" style="color:#FF8C00">[ H5-Index : 18 ]</font>
            </strong>
            <!-- <br> -->
            <!-- <font color="red" size="3"><strong>(Oral Presentation)</strong></font> -->
          </font>
          <br>
          <p></p>
          <a href="javascript:void(0);" onclick="myFunction('icfhr_abs')">
            <font size="3">Abstract</font>
          </a> /
          <!-- <a href="https://github.com/sairajk/Thumbnail-Generation" target="_blank">
                    <font size="3">Code</font>
                  </a> / -->
          <a href="https://arxiv.org/abs/1806.07072" target="_blank">
            <font size="3">arXiv</font>
          </a> /
          <a href="javascript:void(0);" onclick="myFunction('icfhr_bib')">
            <font size="3">BibTex</font>
          </a>
          <p></p>
          <div id="icfhr_abs" style="display:none; text-align:justify;min-width:350px;">
            <font size="3">
              <em>
                Identifying crime for forensic investigating teams
                when crimes involve people of different nationals is challenging.
                This paper proposes a new method for ethnicity (nationality)
                identification based on Cloud of Line Distribution (COLD)
                features of handwriting components. The proposed method, at
                first, explores tangent angle for the contour pixels in each row
                and the mean of intensity values of each row in an image for
                segmenting text lines. For segmented text lines, we use tangent
                angle and direction of base lines to remove rule lines in the
                image. We use polygonal approximation for finding dominant
                points for contours of edge components. Then the proposed
                method connects the nearest dominant points of every dominant
                point, which results in line segments of dominant point pairs. For
                each line segment, the proposed method estimates angle and
                length, which gives a point in polar domain. For all the line
                segments, the proposed method generates dense points in polar
                domain, which results in COLD distribution. As character
                component shapes change, according to nationals, the shape of
                the distribution changes. This observation is extracted based on
                distance from pixels of distribution to Principal Axis of the
                distribution. Then the features are subjected to an SVM classifier
                for identifying nationals. Experiments are conducted on a
                complex dataset, which show the proposed method is effective
                and outperforms the existing method.
              </em>
            </font>
          </div>
          <div id="icfhr_bib" style="font-family:Courier;display:none;min-width:350px;">
            <font size="2">
              <br>
              @inproceedings{nag2018new,<br>
              title={New COLD Feature Based Handwriting Analysis for Enthnicity/Nationality Identification},<br>
              author={Nag, Sauradip and Shivakumara, Palaiahnakote and Wu, Yirui and Pal, Umapada and Lu,
              Tong},<br>
              booktitle={2018 16th International Conference on Frontiers in Handwriting Recognition
              (ICFHR)},<br>
              pages={523--527},<br>
              year={2018},<br>
              organization={IEEE}<br>
              }
            </font>
          </div>
        </td>
      </tr>


      <!-- WACV 2019 - TEXTURE PAPER -->
      <tr onmouseout="cicba_stop()" onmouseover="cicba_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='cicba_image'><img src='images/cicba_2_re.png'></div>
            <img src='images/cicba_re.png'>
          </div>
          <script type="text/javascript">
            function cicba_start() {
              document.getElementById('cicba_image').style.opacity = "1";
            }

            function cicba_stop() {
              document.getElementById('cicba_image').style.opacity = "0";
            }
            cicba_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://link.springer.com/chapter/10.1007/978-981-13-2345-4_5" target="_blank">
            <papertitle>
              <font size="3">Offline Extraction of Indic Regional Language from Natural Scene Image Using Text
                Segmentation and Deep Convolutional Sequence</font>
            </papertitle>
          </a>
          <br>
          <p></p>
          <font size="3">
            <strong>
              <font size="3">Sauradip Nag</font>
            </strong>,
            <a href="https://www.surrey.ac.uk/people/ayan-kumar-bhunia" target="_blank">
              <font size="3">Pallab Ganguly</font>
            </a>,
            <a href="https://www.linkedin.com/in/pranay-mukherjee-701588137/" target="_blank">
              <font size="3">Sumit Roy</font>
            </a>,
            <a href="https://www.linkedin.com/in/pranay-mukherjee-701588137/" target="_blank">
              <font size="3">Sourab Jha</font>
            </a>,
            <a href="https://www.linkedin.com/in/pranay-mukherjee-701588137/" target="_blank">
              <font size="3"> Krishna Bose</font>
            </a>,
            <a href="https://www.linkedin.com/in/pranay-mukherjee-701588137/" target="_blank">
              <font size="3"> Abhishek Jha</font>
            </a>,
            Abhirup Das,
            <a href="https://www.iitr.ac.in/departments/CSE/pages/People+Faculty+Partha_Pratim_Roy.html"
              target="_blank">
              <font size="3"> Kousik Dasgupta</font>
            </a>
            <br>
            <em>International Conference on Computational Intelligence, Communications, and Business Analytics
              (CICBA)</em>, 2018
            <br>
            <em> Kalyani, India <em>
          </font>
          <br>
          <p></p>
          <a href="javascript:void(0);" onclick="myFunction('cicba_abs')">
            <font size="3">Abstract</font>
          </a> /
          <a href="https://arxiv.org/abs/1806.06208" target="_blank">
            <font size="3">arXiv</font>
          </a> /
          <a href="javascript:void(0);" onclick="myFunction('cicba_bib')">
            <font size="3">BibTex</font>
          </a>
          <p></p>
          <div id="cicba_abs" style="display:none; text-align:justify;min-width:350px;">
            <font size="3">
              <em>
                Regional language extraction from a natural scene image is always a challenging proposition due
                to its dependence on the text information extracted from Image. Text Extraction on the other
                hand varies on different lighting condition, arbitrary orientation, inadequate text information,
                heavy background influence over text and change of text appearance. This paper presents a novel
                unified method for tackling the above challenges. The proposed work uses an image correction and
                segmentation technique on the existing Text Detection Pipeline an Efficient and Accurate Scene
                Text Detector (EAST). EAST uses standard PVAnet architecture to select features and non maximal
                suppression to detect text from image. Text recognition is done using combined architecture of
                MaxOut convolution neural network (CNN) and Bidirectional long short term memory (LSTM) network.
                After recognizing text using the Deep Learning based approach, the native Languages are
                translated to English and tokenized using standard Text Tokenizers. The tokens that very likely
                represent a location is used to find the Global Positioning System (GPS) coordinates of the
                location and subsequently the regional languages spoken in that location is extracted. The
                proposed method is tested on a self generated dataset collected from Government of India dataset
                and experimented on Standard Dataset to evaluate the performance of the proposed technique.
                Comparative study with a few state-of-the-art methods on text detection, recognition and
                extraction of regional language from images shows that the proposed method outperforms the
                existing methods.
              </em>
            </font>
          </div>
          <div id="cicba_bib" style="font-family:Courier;display:none;min-width:350px;">
            <font size="2">
              <br>
              @incollection{nag2018offline,<br>
              title={Offline Extraction of Indic Regional Language from Natural Scene Image Using Text
              Segmentation and Deep Convolutional Sequence},<br>
              author={Nag, Sauradip and Ganguly, Pallab Kumar and Roy, Sumit and Jha, Sourab and Bose, Krishna
              and Jha, Abhishek and Dasgupta, Kousik},<br>
              booktitle={Methodologies and Application Issues of Contemporary Computing Framework},<br>
              pages={49--68},<br>
              year={2018},<br>
              publisher={Springer}<br>
              }
            </font>
          </div>
        </td>
      </tr>


      <!-- <tr onmouseout="loss_stop()" onmouseover="loss_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='loss_image'><img src='images/loss_after.png'></div>
                <img src='images/loss_before.png'>
              </div>
              <script type="text/javascript">
                function loss_start() {
                  document.getElementById('loss_image').style.opacity = "1";
                }

                function loss_stop() {
                  document.getElementById('loss_image').style.opacity = "0";
                }
                loss_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=1xpZ0fL9h1y9RfcTyPgVkxUrF3VwdkBvq">
                <papertitle>A General and Adaptive Robust Loss Function</papertitle>
              </a>
              <br>
              <strong>Jonathan T. Barron</strong>
              <br>
              <em>CVPR</em>, 2019 &nbsp <font color="red"><strong>(Oral Presentation, Best Paper Award Finalist)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/1701.03077">arxiv</a> /
              <a href="https://drive.google.com/open?id=1HNveL7xSNh6Ss7sxLK8Mw2L1Fc-rRhL4">supplement</a> /
              <a href="https://youtu.be/BmNKbnF69eY">video</a> /
              <a href="https://www.youtube.com/watch?v=4IInDT_S0ow&t=37m22s">talk</a> /
              <a href="https://drive.google.com/file/d/1GzRYRIfLHvNLT_QwjHoBjHkBbs3Nbf0x/view?usp=sharing">slides</a> /
              <a href="https://github.com/google-research/google-research/tree/master/robust_loss">tensorflow code</a> /
              <a href="https://github.com/jonbarron/robust_loss_pytorch">pytorch code</a> /
              <a href="data/BarronCVPR2019_reviews.txt">reviews</a> /
              <a href="data/BarronCVPR2019.bib">bibtex</a>
              <p></p>
              <p>A single robust loss function is a superset of many other common robust loss functions, and allows training to automatically adapt the robustness of its own loss.</p>
            </td>
          </tr> -->

    </tbody>
  </table>

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <!-- <td style="padding:20px;padding-top:8px;padding-bottom:5px;width:100%;vertical-align:middle">
                  <heading>
                    <font size="4">Projects</font>
                  </heading>
                </td> -->
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>
            <font size="5">Projects</font>
          </heading>
        </td>
      </tr>
    </tbody>
  </table>
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>

      <tr onmouseout="proj_sisr_stop()" onmouseover="proj_sisr_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='proj_sisr_image'><img src='images/talvt_re.gif'></div>
            <img src='images/talvt_re.gif'>
          </div>
          <script type="text/javascript">
            function proj_sisr_start() {
              document.getElementById('proj_sisr_image').style.opacity = "1";
            }

            function proj_sisr_stop() {
              document.getElementById('proj_sisr_image').style.opacity = "0";
            }
            proj_sisr_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle;text-align:justify">
          <a href="https://github.com/sauradip/lightweight_saliency_detection" target="_blank">
            <papertitle>
              <font size="3">Temporal Action Localization Visualization Tool</font>
            </papertitle>
          </a>
          <br><br>
          <font size="3">
            <!-- Image super resolution is the process of increasing the resolution of a Low Resolution (LR) image by generating pixels which interpolate best between the given LR image and the required High Resolution (HR) image. The code was written using Keras (TF backend) in Python3 and comes with an easy to use interface. -->
            <!-- The task has numerous applications like in Satellite Imaging, Surveillance, Medical image processing, Biometrics, etc. -->
            Impressive progress has been reported in recent literature for action recognition. This trend motivates
            another challenging topic - temporal action localization: given a long untrimmed video, “when does a
            specific action start and end?” This problem is important
            because real applications usually involve long untrimmed videos, which can be highly unconstrained in space
            and
            time, and one video can contain multiple action instances plus background scenes or other activities.
            However, there is practically no code available to visualize
            the results and compare with the ground truth for a given video. The only thing that is available currently
            is the quantitaive results
            which is evaluated via the codes given by respective dataset. This is a visualization tool designed to
            bridge this gap and
            observe the performance of any pytorch model on Temporal Activity Localization. This has been designed in
            HTML, CSS , JS and python.

          </font>
          <br>
          <p></p>
          <a href="https://github.com/sauradip/action_localization_visualization" target="_blank">
            <font size="3">Code</font>
          </a>
          <!-- <a href="https://arxiv.org/abs/1912.03641" target="_blank">
            <font size="3">Arxiv</font>
          </a> -->
          <p></p>
        </td>

      </tr>






      <tr onmouseout="proj_sisr_stop()" onmouseover="proj_sisr_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='proj_sisr_image'><img src='images/robot_loco_short.gif'></div>
            <img src='images/robot_loco_short.gif'>
          </div>
          <script type="text/javascript">
            function proj_sisr_start() {
              document.getElementById('proj_sisr_image').style.opacity = "1";
            }

            function proj_sisr_stop() {
              document.getElementById('proj_sisr_image').style.opacity = "0";
            }
            proj_sisr_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle;text-align:justify">
          <a href="https://github.com/sauradip/vision_based_robot_navigation" target="_blank">
            <papertitle>
              <font size="3">Computer Vision based Robot Locomotion</font>
            </papertitle>
          </a>
          <br><br>
          <font size="3">
            <!-- Image super resolution is the process of increasing the resolution of a Low Resolution (LR) image by generating pixels which interpolate best between the given LR image and the required High Resolution (HR) image. The code was written using Keras (TF backend) in Python3 and comes with an easy to use interface. -->
            <!-- The task has numerous applications like in Satellite Imaging, Surveillance, Medical image processing, Biometrics, etc. -->
            Vision-based robot navigation has long been a fundamental goal in both robotics and computer vision
            research. However, we do not require all semantic labelling of a particular environment for a robot
            to move.
            It requires only the floor part of a environment to navigate its path as we are dealing with ground
            based robots. Depth information is particularly useful in predicting how much the robot can move in
            a particular direction. Hence, the marriage between both the vision tasks provides us a free space
            map which enables the robot to move freely in a given direction.Hence, we have designed a novel
            motion control algorithm
            which enables the robot to naviagte through obstacles in its path. This is implemented in Pytorch.
          </font>
          <br>
          <p></p>
          <a href="https://github.com/sauradip/vision_based_robot_navigation" target="_blank">
            <font size="3">Code</font>
          </a>
          <!-- <a href="https://github.com/YapengTian/Single-Image-Super-Resolution" target="_blank">
                    <font size="3">Papers on Super Resolution</font>
                  </a> -->
          <p></p>
        </td>

      </tr>

      <tr onmouseout="proj_sisr_stop()" onmouseover="proj_sisr_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='proj_sisr_image'><img src='images/robot.gif'></div>
            <img src='images/robot.gif'>
          </div>
          <script type="text/javascript">
            function proj_sisr_start() {
              document.getElementById('proj_sisr_image').style.opacity = "1";
            }

            function proj_sisr_stop() {
              document.getElementById('proj_sisr_image').style.opacity = "0";
            }
            proj_sisr_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle;text-align:justify">
          <a href="https://github.com/sauradip/autonomous_robot_locomotion" target="_blank">
            <papertitle>
              <font size="3">Autonomous Robot Locomotion in prespecified path</font>
            </papertitle>
          </a>
          <br><br>
          <font size="3">
            <!-- Image super resolution is the process of increasing the resolution of a Low Resolution (LR) image by generating pixels which interpolate best between the given LR image and the required High Resolution (HR) image. The code was written using Keras (TF backend) in Python3 and comes with an easy to use interface. -->
            <!-- The task has numerous applications like in Satellite Imaging, Surveillance, Medical image processing, Biometrics, etc. -->
            Robotics have helped humans greatly in achieving everyday tasks. Robots are designed to work in any
            environment and perform task on behalf of humans. They operate under real-world and real-time
            constraints where sensors and effectors with specific physical characteristics have to be
            controlled. In many cases, those robots are controlled manually to move from one destination to
            another. An Unmanned Ground Vehicle (UGV) is a vehicle that operates while in contact with the
            ground and without an onboard human presence. We have used
            one of such robots to demonstrate the custom path which user can choose. Currently we have
            implemented 2 such custom paths. This has been implemented in Python and ROS.
          </font>
          <br>
          <p></p>
          <a href="https://github.com/sauradip/autonomous_robot_locomotion" target="_blank">
            <font size="3">Code</font>
          </a>
          <!-- <a href="https://github.com/YapengTian/Single-Image-Super-Resolution" target="_blank">
                    <font size="3">Papers on Super Resolution</font>
                  </a> -->
          <p></p>
        </td>

      </tr>

      <tr onmouseout="proj_sisr_stop()" onmouseover="proj_sisr_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='proj_sisr_image'><img src='images/proj_5_re.gif'></div>
            <img src='images/robot.gif'>
          </div>
          <script type="text/javascript">
            function proj_sisr_start() {
              document.getElementById('proj_sisr_image').style.opacity = "1";
            }

            function proj_sisr_stop() {
              document.getElementById('proj_sisr_image').style.opacity = "0";
            }
            proj_sisr_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle;text-align:justify">
          <a href="https://github.com/sauradip/lightweight_saliency_detection" target="_blank">
            <papertitle>
              <font size="3">Light-weight Salient Object Detection</font>
            </papertitle>
          </a>
          <br><br>
          <font size="3">
            <!-- Image super resolution is the process of increasing the resolution of a Low Resolution (LR) image by generating pixels which interpolate best between the given LR image and the required High Resolution (HR) image. The code was written using Keras (TF backend) in Python3 and comes with an easy to use interface. -->
            <!-- The task has numerous applications like in Satellite Imaging, Surveillance, Medical image processing, Biometrics, etc. -->
            Salient object detection is a prevalent computer vision task that has applications ranging from abnormality
            detection to abnormality processing. Context modelling is an important criterion in the domain of saliency
            detection. A global context helps in determining the salient object in a given image by contrasting away
            other objects in the global view of the scene. However, the local context features detects the boundaries of
            the salient object with higher accuracy in a given region. To incorporate the best of both worlds, our
            proposed SaLite model uses both global and local contextual features. It is an encoder-decoder based
            architecture in which the encoder uses a lightweight SqueezeNet and decoder is modelled using convolution
            layers. This has been implemented in PyTorch.

          </font>
          <br>
          <p></p>
          <a href="https://github.com/sauradip/lightweight_saliency_detection" target="_blank">
            <font size="3">Code</font>
          </a> /
          <a href="https://arxiv.org/abs/1912.03641" target="_blank">
            <font size="3">Arxiv</font>
          </a>
          <p></p>
        </td>

      </tr>



      <tr onmouseout="proj_sisr_stop()" onmouseover="proj_sisr_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='proj_sisr_image'><img src='images/proj_3.gif'></div>
            <img src='images/robot.gif'>
          </div>
          <script type="text/javascript">
            function proj_sisr_start() {
              document.getElementById('proj_sisr_image').style.opacity = "1";
            }

            function proj_sisr_stop() {
              document.getElementById('proj_sisr_image').style.opacity = "0";
            }
            proj_sisr_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle;text-align:justify">
          <a href="https://github.com/sauradip/Boundary-Growing-in-Python" target="_blank">
            <papertitle>
              <font size="3">Boundary Growing Algorithm for Recovery of Torso from Corrupt Face</font>
            </papertitle>
          </a>
          <br><br>
          <font size="3">
            <!-- Image super resolution is the process of increasing the resolution of a Low Resolution (LR) image by generating pixels which interpolate best between the given LR image and the required High Resolution (HR) image. The code was written using Keras (TF backend) in Python3 and comes with an easy to use interface. -->
            <!-- The task has numerous applications like in Satellite Imaging, Surveillance, Medical image processing, Biometrics, etc. -->
            Automatic face detection has been intensively studied for human related recognition systems.
            However, there has been a very little work in recovering face from a corrupted face image. We have
            designed a boundary growing algorithm where we incrementally grew the boundary of the corrupted face
            image and passed into HAAR cascade classifier to get confidence score. We kept on doing until we
            reach the maximum confidence. After that we used tailor measurements to recover the torso part of
            the human. This has been implemented using OpenCV and Python.

          </font>
          <br>
          <p></p>
          <a href="https://github.com/sauradip/Boundary-Growing-in-Python" target="_blank">
            <font size="3">Code</font>
          </a>
          <!-- <a href="https://github.com/YapengTian/Single-Image-Super-Resolution" target="_blank">
                    <font size="3">Papers on Super Resolution</font>
                  </a> -->
          <p></p>
        </td>

      </tr>


      <tr onmouseout="proj_sisr_stop()" onmouseover="proj_sisr_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='proj_sisr_image'><img src='images/proj_4.gif'></div>
            <img src='images/proj_4.gif'>
          </div>
          <script type="text/javascript">
            function proj_sisr_start() {
              document.getElementById('proj_sisr_image').style.opacity = "1";
            }

            function proj_sisr_stop() {
              document.getElementById('proj_sisr_image').style.opacity = "0";
            }
            proj_sisr_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle;text-align:justify">
          <a href="https://github.com/sauradip/BaseLine-Remover" target="_blank">
            <papertitle>
              <font size="3">BaseLine Remover from Doument Images</font>
            </papertitle>
          </a>
          <br><br>
          <font size="3">
            <!-- Image super resolution is the process of increasing the resolution of a Low Resolution (LR) image by generating pixels which interpolate best between the given LR image and the required High Resolution (HR) image. The code was written using Keras (TF backend) in Python3 and comes with an easy to use interface. -->
            <!-- The task has numerous applications like in Satellite Imaging, Surveillance, Medical image processing, Biometrics, etc. -->
            This a small Matlab Implementation for Removal of Base Line from document using Edge Directional
            Kernel stated in paper " Edge enhancement algorithm for low-dose X-ray fluoroscopic imaging " by Lee
            et al. Here Baselines are Removed using Edge Directional Kernel . BaseLine Removal is an important
            topic in Document Image Analysis . In this paper Lee et al proposed removal of Noise from XRAY
            Images using Edge Directional Kernel and High Pass Filter but since our Noise is only Baseline we
            used a clever trick to implement only the Edge Directional Kernel and the reuslts are quite neat.
            This method works well for Half Page Document and Cropped Line Images , however if full page images
            are preprocessed then it may work pretty well.

          </font>
          <br>
          <p></p>
          <a href="https://github.com/sauradip/BaseLine-Remover" target="_blank">
            <font size="3">Code</font>
          </a> /
          <a href="https://github.com/YapengTian/Single-Image-Super-Resolution" target="_blank">
            <font size="3">Paper</font>
          </a>
          <p></p>
        </td>

      </tr>
    </tbody>
  </table>




  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>
            <font size="5">Past Collaborators</font>
          </heading>
        </td>
      </tr>
      <tr>
        <td style="padding:0px;width:75%;vertical-align:middle">
          <!-- Projects -->

          <div class="wrapper">

            <img src="images/Umapada_Pal.jpg" class="image--cover">

            <img src="images/shivakumara.jpeg" alt="" class="image--cover" />

            <img src="images/raghavendra.jpg" alt="" class="image--cover" />

            <img src="images/Michael-Blumenstein.jpg" alt="" class="image--cover" />
          </div>
      </tr>
      <tr>
        <td style="padding:0px;width:75%;vertical-align:middle">
          <!-- Projects -->
          <!-- <div class="wrapper">

        <span style="padding-left:30px;padding-right:30px;">Umapada Pal</span>
        <span style="padding-left:30px;padding-right:30px;"></span>
        <span style="padding-left:30px;padding-right:30px;">Palaiahnakote Shivakumara</span>
        <span style="padding-left:30px;padding-right:30px;"></span>
        <span style="padding-left:30px;padding-right:30px;">Raghavendra Ramachandra</span>
        <span style="padding-left:30px;padding-right:30px;"></span>
        <span style="padding-left:30px;padding-right:30px;">Michael Blumenstein</span>

      </div> -->
          <div class="demo" style="padding:5px;">
            <span class="collab"><a style="font-size:16px"
                href="https://scholar.google.co.in/citations?user=2_z_CogAAAAJ&hl=en">Umapada Pal</a></span>
            <span class="collab" style="padding-left:35px"><a style="font-size:16px"
                href="https://scholar.google.co.in/citations?user=XfpbOc4AAAAJ&hl=en">P Shivakumara</a></span>
            <span class="collab"><a style="font-size:16px"
                href="https://scholar.google.co.in/citations?user=OIYIrmIAAAAJ&hl=en">R Ramachandra</a></span>
            <span class="collab"><a style="font-size:16px"
                href="https://scholar.google.co.in/citations?user=4m2G-H8AAAAJ&hl=en">Michael
                Blumenstein</a></span>
          </div>
          <div class="wrapper">

            <img src="images/TongLu.jpg" class="image--cover">

            <img src="images/parthapratim.jpg" alt="" class="image--cover" />

            <img src="images/mohan_kankanhalli.jpg" alt="" class="image--cover" />

            <img src="images/kd_sir_re.jpg" alt="" class="image--cover" />

          </div>
          <!-- <div class="wrapper">

        <span style="padding-left:30px;padding-right:30px;">Umapada_Pal</span>
        <span style="padding-left:30px;padding-right:30px;"></span>
        <span style="padding-left:30px;padding-right:30px;">Umapada_Pal</span>
        <span style="padding-left:30px;padding-right:30px;"></span>
        <span style="padding-left:30px;padding-right:30px;">Umapada_Pal</span>
        <span style="padding-left:30px;padding-right:30px;"></span>
        <span style="padding-left:30px;padding-right:30px;">Umapada_Pal</span>

      </div> -->
          <div class="demo">
            <span class="collab" style="padding-left:30px"><a style="font-size:16px"
                href="https://cs.nju.edu.cn/_upload/tpl/00/cd/205/template205/Publications.html">Lu
                Tong</a></span>
            <span class="collab" style="padding-left:32px"><a style="font-size:16px"
                href="https://scholar.google.co.in/citations?user=moDpyKkAAAAJ&hl=en">Partha Roy</a></span>
            <span class="collab" style="padding-left:-10px;"><a style="font-size:16px"
                href="https://scholar.google.com/citations?user=6Lx_eowAAAAJ&hl=th">M Kankanhalli</a></span>
            <span class="collab"><a style="font-size:16px"
                href="https://scholar.google.co.in/citations?user=63DEdC0AAAAJ&hl=en">Kousik Dasgupta</a></span>
          </div>
      </tr>
      <!--  -->
    </tbody>
  </table>
  <!-- Footer - Template Credits -->
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:0px">
          <br>
          <p style="text-align:center;font-size:medium;">
            Visitor Map
            <!--                     <a href="https://jonbarron.info/" target="_blank">Dr. Jon Barron</a> -->
          </p>
        </td>
      </tr>
    </tbody>
  </table>

  <center>
    <script type='text/javascript' id='clustrmaps'
      src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=400&t=n&d=NLN3vt4IdIZciRC65gs2Q4eyOFL2xjRooRB98TlWaI0&co=118fe8'></script>
  </center>



  </td>
  </tr>
  </table>
  <br>
  <table align=center width=900px>
    <tr>
      <td align=center width=900px>
        <center><img src="./images/website_logo.png" height="90x" width="800x"></img><br></center>

      </td>
    </tr>
    <br>
  </table>
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:0px">
          <br>

          <p style="text-align:center;font-size:small;">
            <!--             Template credits :
            <a href="https://jonbarron.info/" target="_blank">Dr. Jon Barron</a>  -->
            Copyright © Sauradip Nag. Last updated Sep 2020 | Template provided by <a href="https://jonbarron.info/"
              target="_blank">Dr. Jon Barron</a>
          </p>
        </td>
      </tr>
    </tbody>
  </table>
</body>

</html>
