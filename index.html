<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Sauradip Nag</title>

  <meta name="author" content="Sauradip Nag">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="stylesheet" type="text/css" href="style.css">
  <link rel="icon" type="image/jpeg" href="images/icon.jpeg">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Sauradip Nag</name>
                    <br>
                    <font size="3"><br />Teaching machines to <br />THINK LIKE HUMANS.</font>
                  </p>
                  <br />
                  <p align="justify">
                    <font size="3">
                      I am a Project Associate in <a href="http://www.cse.iitm.ac.in/~vplab/" target="_blank">
                        <font size="3">Visualization and Percepion Lab </font>
                      </a>, IIT Madras working on a DRDO Project which involves finding out hidden location from a
                      unknown environment.
                    </font>
                  </p>
                  <p align="justify">
                    <font size="3">
                      I did my Bachelor's in Computer Science and Engineering from Kalyani Government Engineering
                      College (KGEC), Kalyani, India.
                      During my undergrad, I got the opportunity to work with <a
                        href="https://www.isical.ac.in/~umapada/" target="_blank">
                        <font size="3">Prof. Umapada Pal</font>
                      </a>, Head of <a href="https://www.isical.ac.in/~cvpr/" target="_blank">
                        <font size="3">Computer Vision & Pattern Recognition Unit</font>
                      </a> at <a href="https://www.isical.ac.in/" target="_blank">
                        <font size="3">Indian Statistical Institute (ISI), Kolkata</font>
                      </a> and <a href="https://umexpert.um.edu.my/shiva" target="_blank">
                        <font size="3">Prof. Palaihnakhote Shivakumara</font>
                      </a> of <a href="https://www.um.edu.my/" target="_blank">
                        <font size="3">University of Malaya , Malyasia</font>
                      </a> on various Computer Vision research problems. I also did my research project under <a
                        href="https://www.iitr.ac.in/departments/CSE/pages/People+Faculty+Partha_Pratim_Roy.html"
                        target="_blank">
                        <font size="3">Prof. Partha Pratim Roy</font>
                      </a> of <a href="https://www.iitr.ac.in/" target="_blank">
                        <font size="3">Indian Institute of Technolgy (IIT), Roorkee</font>
                      </a> where I worked on Facial Microexpression. I completed my bachelor's thesis on <i>Interacting
                        with Softwares using Gestures</i> under the guidance of Prof. Koushik Dasgupta of KGEC and Mr.
                      Tamojit Chatterjee of Indeed Inc.
                    </font>
                  </p>
                  <br>
                  <p style="text-align:center">
                    <a href="mailto:sauradipnag95@gmail.com" target="_blank">
                      <font size="3">Email</font>
                    </a> &nbsp/&nbsp
                    <a href="Sauradip_Nag_CV.pdf" target="_blank">CV</a> &nbsp/&nbsp
                    <!-- <a href="data/JonBarron-bio.txt">Biography</a> &nbsp/&nbsp -->
                    <a href="https://scholar.google.com/citations?user=hlkMCO4AAAAJ&hl=en" target="_blank">
                      <font size="3">Google Scholar</font>
                    </a> &nbsp/&nbsp
                    <a href="https://www.linkedin.com/in/sauradip-nag-1b6059112/" target="_blank">
                      <font size="3"> LinkedIn </font>
                    </a> &nbsp/&nbsp
                    <a href="https://github.com/sauradip" target="_blank">
                      <font size="3"> GitHub </font>
                    </a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/profile.jpg" target="_blank"><img style="width:100%;max-width:100%"
                      alt="Profile Photo" src="images/rsz_saura6_crop.png" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- RESEARCH -->

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle;text-align:justify">
                  <heading>
                    <font size="5">Research Interests</font>
                  </heading>
                  <p>
                    <font size="3">
                      I am broadly interested in the field of Computer Vision and Deep Learning. Particularly, I like to
                      think upon <b>Visual Scene Understanding (VSU)</b> from images and videos, effective methods of
                      <b>Domain Adaptation & Transfer Learning</b> for VSU, building <b>systems that learn with minimal
                        or no supervision</b> and <b>systems that generalize well</b> in real and diverse scenarios.
                      I am also open to any topic that would be interesting or fun to explore and pursue.
                    </font>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>


          <!-- Updates on Recent Activities  -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td
                  style="padding:20px;padding-top:5px;padding-bottom:5px;width:100%;vertical-align:middle;text-align:justify">
                  <heading>
                    <font size="5">Updates</font>
                  </heading>
                  <p>
                    <font size="3">
                      <!-- <a class="button" href="#" style="color:#FFA500"><strong style="font-size:12px">New</strong></a> -->
                      <!-- <button type="submit" class="button">Click me!</button> -->
                      <ul style="padding-inline-start:0px;list-style-type:none;">
                        <!--                         <li> <a class="button" href="#" style="color:#FFA500"><strong style="font-size:12px">New</strong></a><em style="font-size:16px;"> 1 paper on Future Frame Depth Prediction communicated to <strong style="font-size:16px">ECCV 2020</strong></em></li> -->
                        <p style="margin-block-start:0px;margin-block-end:3px"></p>
                        <li> <a class="button" href="#" style="color:#FFA500"><strong
                              style="font-size:12px">New</strong></a> <strong style="font-size:15px">05/2020 : </strong>
                          <em style="font-size:16px;"> <strong>1</strong> paper got accepted in <strong
                              style="font-size:16px"> <a href="https://www.journals.elsevier.com/pattern-recognition"
                                style="font-size:16px">Pattern Recognition, Elsevier</a> </strong></em></li>
                        <p style="margin-block-start:0px;margin-block-end:3px"></p>
                        <li> <a class="button" href="#" style="color:#FFA500"><strong
                              style="font-size:12px">New</strong></a> <strong style="font-size:15px">04/2020 : </strong>
                          <em style="font-size:16px;"> I am joining <strong style="font-size:16px"> SketchX Lab,
                              University of Surrey</strong> as a PhD student in
                            September 2020</em></li>
                        <p style="margin-block-start:0px;margin-block-end:3px"></p>
                        <li> <a class="button" href="#" style="color:#FFA500"><strong
                              style="font-size:12px">New</strong></a> <strong style="font-size:15px">11/2019 : </strong>
                          <em style="font-size:16px;"> Our paper titled
                            "What's There in The Dark" has been selected in <strong style="font-size:16px"><a
                                style="font-size:16px" href="http://2019.ieeeicip.org/?action=page3&id=9">TOP 10%
                                Papers</a></strong> in <strong style="font-size:16px">ICIP19</strong></em></li>
                        <p style="margin-block-start:0px;margin-block-end:3px"></p>
                        <li style="padding-left:38px;"> <em style="font-size:16px;"> <strong
                              style="font-size:15px">10/2019 : </strong> <strong>1</strong> paper on <strong
                              style="font-size:16px">Light-weight Saliency Detection</strong> is uploaded on Arxiv</em>
                        </li>
                      </ul>
                    </font>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>




          <!-- PUBLICATIONS -->

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;padding-top:5px;padding-bottom:3px;width:100%;vertical-align:middle">
                  <heading>
                    <font size="5">Publications</font>
                  </heading>
                </td>
              </tr>
              <tr>
                <td
                  style="padding:20px;padding-top:5px;padding-bottom:5px;padding-bottom: 0px;width:100%;vertical-align:middle">
                  <heading>
                    <font size="4">Journals</font>
                  </heading>
                </td>
              </tr>
            </tbody>
          </table>

          <script>
            function myFunction(pub_name) {
              var x = document.getElementById(pub_name);
              if (x.style.display === 'none') {
                x.style.display = 'block';
              } else {
                x.style.display = 'none';
              }
            }
          </script>





          <!-- JOURNAL SECTION        -->

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <!-- UNDER REVIEW - BIB NUMBER DETECTION -->
              <tr onmouseout="underreview_0_stop()" onmouseover="underreview_0_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='underreview_0_image'><img src='images/pr_after_re.png'></div>
                    <img src='images/pr_icon(1).png'>
                  </div>
                  <script type="text/javascript">
                    function underreview_0_start() {
                      document.getElementById('underreview_0_image').style.opacity = "1";
                    }

                    function underreview_0_stop() {
                      document.getElementById('underreview_0_image').style.opacity = "0";
                    }
                    underreview_0_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="TODO" target="_blank">
                    <papertitle>
                      <font size="3">A New Unified Method for Detecting Text from Marathon Runners and Sports Players in
                        Video</font>
                    </papertitle>
                  </a>
                  <br>
                  <font size="3">
                    <strong>
                      <font size="3">Sauradip Nag</font>
                    </strong>,
                    <a href="https://sudip.info/" target="_blank">
                      <font size="3">P Shivakumara</font>
                    </a>,
                    <a href="https://www.isical.ac.in/~ujjwal/" target="_blank">
                      <font size="3">Umapada Pal</font>
                    </a>,
                    <a href="https://www.isical.ac.in/~ujjwal/" target="_blank">
                      <font size="3">Tong Lu</font>
                    </a>,
                    <a href="https://www.isical.ac.in/~ujjwal/" target="_blank">
                      <font size="3">Michael Blumenstein</font>
                    </a>
                    <br>
                    <em><strong>
                        <font size="3" style="color:#105c24">Pattern Recognition, Elsevier</font>
                        <font size="3" style="color:#0911eb"> [IF : 5.898]</font>
                      </strong></em><em>(Yet to be online)</em>
                  </font>
                  <br>
                  <p></p>
                  <a href="javascript:void(0);" onclick="myFunction('pr_abs')">
                    <font size="3">Abstract</font>
                  </a> /
                  <a href="https://github.com/sauradip/PR_scene_text" target="_blank">
                    <font size="3">Code</font>
                  </a> /
                  <!-- <a href="https://arxiv.org/abs/1811.01401" target="_blank"><font size="3">arXiv</font></a> / -->
                  <a href="javascript:void(0);" onclick="myFunction('pr_bib')">
                    <font size="3">BibTex</font>
                  </a>
                  <p></p>
                  <div id="pr_abs" style="display:none; text-align:justify;min-width:350px;">
                    <font size="3">
                      <em>
                        Detecting text located on the torsos of marathon runners and sports players in video is a
                        challenging
                        issue due to poor quality and adverse effects caused by flexible/colorful clothing, and
                        different
                        structures of human bodies or actions. This paper presents a new unified method for tackling the
                        above
                        challenges. The proposed method fuses gradient magnitude and direction coherence of text pixels
                        in a
                        new way for detecting candidate regions. Candidate regions are used for determining the number
                        of
                        temporal frame clusters obtained by K-means clustering on frame differences. This process in
                        turn
                        detects key frames. The proposed method explores Bayesian probability for skin portions using
                        color
                        values at both pixel and component levels of temporal frames, which provides fused images with
                        skin
                        components. Based on skin information, the proposed method then detects faces and torsos by
                        finding
                        structural and spatial coherences between them. We further propose adaptive pixels linking a
                        deep
                        learning model for text detection from torso regions. The proposed method is tested on our own
                        dataset
                        collected from marathon/sports video and three standard datasets, namely, RBNR, MMM and R-ID of
                        marathon images, to evaluate the performance. In addition, the proposed method is also tested on
                        the
                        standard natural scene datasets, namely, CTW1500 and MS-COCO text datasets, to show the
                        objectiveness
                        of the proposed method. A comparative study with the state-of-the-art methods on bib number/text
                        detection of different datasets shows that the proposed method outperforms the existing methods
                      </em>
                    </font>
                  </div>
                  <div id="pr_bib" style="font-family:Courier;display:none;min-width:350px;">
                    <font size="2">
                      <br>
                      <!-- @article{kishore2019cluenet,<br>
                      title={ClueNet: A Deep Framework for Occluded Pedestrian Pose Estimation},<br>
                      author={Kishore, Perla Sai Raj and Das, Sudip and Mukherjee, Partha Sarathi and Bhattacharya,
                      Ujjwal},<br>
                      year={2019}<br>
                      } -->
                      @inproceedings{nag2019s,<br>
                      title={What’s There in the Dark},<br>
                      author={Nag, Sauradip and Adak, Saptakatha and Das, Sukhendu},<br>
                      booktitle={2019 IEEE International Conference on Image Processing (ICIP)},<br>
                      pages={2996--3000},<br>
                      year={2019},<br>
                      organization={IEEE}<br>
                      }
                    </font>
                  </div>

                </td>
              </tr>
            </tbody>
          </table>




          <!-- CONFERENCE SECTION -->


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;padding-top:3px;padding-bottom:5px;width:100%;vertical-align:middle">
                  <heading>
                    <font size="4">Conference</font>
                  </heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <!-- UNDER REVIEW - POSE ESTIMATION -->
              <tr onmouseout="icip_stop()" onmouseover="icip_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='icip_image'><img src='images/seg_output_re.png'></div>
                    <img src='images/night_input_re.png'>
                  </div>
                  <script type="text/javascript">
                    function icip_start() {
                      document.getElementById('icip_image').style.opacity = "1";
                    }

                    function icip_stop() {
                      document.getElementById('icip_image').style.opacity = "0";
                    }
                    icip_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/abstract/document/8803299" target="_blank">
                    <papertitle>
                      <font size="3">What's There in the Dark
                      </font>
                    </papertitle>
                  </a>
                  <br>
                  <p></p>
                  <font size="3">
                    <strong>
                      <font size="3">Sauradip Nag</font>
                    </strong>,
                    <a href="https://sudip.info/" target="_blank">
                      <font size="3">Saptakatha Adak</font>
                    </a>,
                    <a href="https://www.isical.ac.in/~ujjwal/" target="_blank">
                      <font size="3">Sukhendu Das</font>
                    </a>
                    <br>
                    <em>International Conference in Image Processing (ICIP)</em>, 2019
                    <font color="red" size="3"><strong>(Spotlight Paper)</strong></font>
                    <br>
                    <em>Taipei, Taiwan </em>
                    <strong>
                      <font size="3" style="color:#FF8C00">[ H5-Index : 45 ]</font>
                    </strong>
                  </font>
                  <br>
                  <p></p>
                  <a href="javascript:void(0);" onclick="myFunction('icip_abs')">
                    <font size="3">Abstract</font>
                  </a> /
                  <a href="https://github.com/sauradip/night_image_semantic_segmentation" target="_blank">
                    <font size="3">Code</font>
                  </a> /
                  <!-- <a href="https://arxiv.org/abs/1811.01401" target="_blank"><font size="3">arXiv</font></a> / -->
                  <a href="javascript:void(0);" onclick="myFunction('icip_bib')">
                    <font size="3">BibTex</font>
                  </a>
                  <p></p>
                  <div id="icip_abs" style="display:none; text-align:justify;min-width:350px;">
                    <font size="3">
                      <em>
                        Scene Parsing is an important cog for modern autonomous driving systems. Most of the works in
                        semantic segmentation pertains to day-time scenes with favourable weather and illumination
                        conditions. In this paper, we propose a novel deep architecture, NiSeNet, that performs semantic
                        segmentation of night scenes using a domain mapping approach of synthetic to real data. It is a
                        dual-channel network, where we designed a Real channel using DeepLabV3+ coupled with an MSE loss
                        to preserve the spatial information. In addition, we used an Adaptive channel reducing the
                        domain gap between synthetic and real night images, which also complements the failures of Real
                        channel output. Apart from the dual channel, we introduced a novel fusion scheme to fuse the
                        outputs of two channels. In addition to that, we compiled a new dataset Urban Night Driving
                        Dataset (UNDD); it consists of 7125 unlabelled day and night images; additionally, it has 75
                        night images with pixel-level annotations having classes equivalent to Cityscapes dataset. We
                        evaluated our approach on the Berkley Deep Drive dataset, the challenging Mapillary dataset and
                        UNDD dataset to exhibit that the proposed method outperforms the state-of-the-art techniques in
                        terms of accuracy and visual quality.
                      </em>
                    </font>
                  </div>
                  <div id="icip_bib" style="font-family:Courier;display:none;min-width:350px;">
                    <font size="2">
                      <br>
                      <!-- @article{kishore2019cluenet,<br>
                      title={ClueNet: A Deep Framework for Occluded Pedestrian Pose Estimation},<br>
                      author={Kishore, Perla Sai Raj and Das, Sudip and Mukherjee, Partha Sarathi and Bhattacharya,
                      Ujjwal},<br>
                      year={2019}<br>
                      } -->
                      @inproceedings{nag2019s,<br>
                      title={What’s There in the Dark},<br>
                      author={Nag, Sauradip and Adak, Saptakatha and Das, Sukhendu},<br>
                      booktitle={2019 IEEE International Conference on Image Processing (ICIP)},<br>
                      pages={2996--3000},<br>
                      year={2019},<br>
                      organization={IEEE}<br>
                      }
                    </font>
                  </div>
                </td>
              </tr>







              <!-- BMVC 2019 - POSE ESTIMATION -->
              <tr onmouseout="icassp_stop()" onmouseover="icassp_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='icassp_image'><img src='images/icassp_2_re.png'></div>
                    <img src='images/icassp_re.png'>
                  </div>
                  <script type="text/javascript">
                    function icassp_start() {
                      document.getElementById('icassp_image').style.opacity = "1";
                    }

                    function icassp_stop() {
                      document.getElementById('icassp_image').style.opacity = "0";
                    }
                    icassp_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/abstract/document/8683737" target="_blank">
                    <papertitle>
                      <font size="3">Facial Micro-Expression Spotting and Recognition using Time Contrasted Feature with
                        Visual Memory</font>
                    </papertitle>
                  </a>
                  <br>
                  <font size="3">
                    <strong>
                      <font size="3">Sauradip Nag</font>
                    </strong>,
                    <a href="https://sudip.info/" target="_blank">
                      <font size="3">Ayan Kumar Bhunia</font>
                    </a>,
                    Aishik Konwer,
                    <a href="https://www.isical.ac.in/~ujjwal/" target="_blank">
                      <font size="3">Partha Pratim Roy</font>
                    </a>
                    <br>
                    <em>International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2019
                    <br>
                    <em>Brighton, United Kingdom </em>
                    <strong>
                      <font size="3" style="color:#FF8C00">[ H5-Index : 80 ]</font>
                    </strong>

                  </font>
                  <br>
                  <p></p>
                  <a href="javascript:void(0);" onclick="myFunction('icassp_abs')">
                    <font size="3">Abstract</font>
                  </a> /
                  <a href="https://arxiv.org/abs/1902.03514" target="_blank">
                    <font size="3">arXiv</font>
                  </a> /
                  <a href="javascript:void(0);" onclick="myFunction('icassp_bib')">
                    <font size="3">BibTex</font>
                  </a>
                  <p></p>
                  <div id="icassp_abs" style="display:none; text-align:justify;min-width:350px;">
                    <font size="3">
                      <em>
                        Facial micro-expressions are sudden involuntary minute muscle movements which reveal true
                        emotions that people try to conceal. Spotting a micro-expression and recognizing it is a major
                        challenge owing to its short duration and intensity. Many works pursued traditional and deep
                        learning based approaches to solve this issue but compromised on learning low level features and
                        higher accuracy due to unavailability of datasets. This motivated us to propose a novel joint
                        architecture of spatial and temporal network which extracts time-contrasted features from the
                        feature maps to contrast out micro-expression from rapid muscle movements. The usage of time
                        contrasted features greatly improved the spotting of micro-expression from inconspicuous facial
                        movements. Also, we include a memory module to predict the class and intensity of the
                        micro-expression across the temporal frames of the micro-expression clip. Our method achieves
                        superior performance in comparison to other conventional approaches on CASMEII dataset.
                      </em>
                    </font>
                  </div>
                  <div id="icassp_bib" style="font-family:Courier;display:none;min-width:350px;">
                    <font size="2">
                      <br>
                      @inproceedings{nag2019facial,<br>
                      title={Facial Micro-Expression Spotting and Recognition using Time Contrasted Feature with Visual
                      Memory},<br>
                      author={Nag, Sauradip and Bhunia, Ayan Kumar and Konwer, Aishik and Roy, Partha Pratim},<br>
                      booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal
                      Processing (ICASSP)},<br>
                      pages={2022--2026},<br>
                      year={2019},<br>
                      organization={IEEE}<br>
                      }<br>
                    </font>
                  </div>
                </td>
              </tr>


              <!-- CVPR 2019 - HANDWRITING RECOGNITION -->
              <tr onmouseout="icdar_stop()" onmouseover="icdar_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='icdar_image'><img src='images/icdar_2_re.png'></div>
                    <img src='images/icdar_re.png'>
                  </div>
                  <script type="text/javascript">
                    function icdar_start() {
                      document.getElementById('icdar_image').style.opacity = "1";
                    }

                    function icdar_stop() {
                      document.getElementById('icdar_image').style.opacity = "0";
                    }
                    icdar_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/abstract/document/8977968" target="_blank">
                    <papertitle>
                      <font size="3">CRNN based Jersey-Bib Number/Text Recognition in Sports and Marathon Images</font>
                    </papertitle>
                  </a>
                  <br>

                  <font size="3">
                    <strong>
                      <font size="3">Sauradip Nag</font>
                    </strong>,
                    <a href="https://www.surrey.ac.uk/people/ayan-kumar-bhunia" target="_blank">
                      <font size="3">Raghavendra Ramachandra</font>
                    </a>,
                    <a href="https://ankanbhunia.github.io/" target="_blank">
                      <font size="3">Palaiahnakote Shivakumara</font>
                    </a>,
                    <a href="https://www.iitr.ac.in/departments/CSE/pages/People+Faculty+Partha_Pratim_Roy.html"
                      target="_blank">
                      <font size="3">Umapada Pal</font>
                    </a>,
                    <a href="https://www.iitr.ac.in/departments/CSE/pages/People+Faculty+Partha_Pratim_Roy.html"
                      target="_blank">
                      <font size="3">Tong Lu</font>
                    </a>,
                    <a href="https://www.iitr.ac.in/departments/CSE/pages/People+Faculty+Partha_Pratim_Roy.html"
                      target="_blank">
                      <font size="3">Mohan Kankanhalli</font>
                    </a>
                    <br>
                    <em>International Conference on Document Analysis and Recognition (ICDAR)</em>, 2019
                    <br>
                    <em>Sydney, Australia </em>
                    <strong>
                      <font size="3" style="color:#FF8C00">[ H5-Index : 26 ]</font>
                    </strong>
                  </font>
                  <br>
                  <p></p>
                  <a href="javascript:void(0);" onclick="myFunction('icdar_abs')">
                    <font size="3">Abstract</font>
                  </a> /
                  <!-- <a href="https://github.com/AyanKumarBhunia/Handwriting_Recogition_using_Adversarial_Learning"
                    target="_blank">
                    <font size="3">Code</font>
                  </a> / -->
                  <!-- <a href="https://arxiv.org/abs/1811.01396" target="_blank" target="_blank">
                    <font size="3">arXiv</font>
                  </a> / -->
                  <a href="javascript:void(0);" onclick="myFunction('icdar_bib')">
                    <font size="3">BibTex</font>
                  </a>
                  <p></p>
                  <div id="icdar_abs" style="display:none; text-align:justify;min-width:350px;">
                    <font size="3">
                      <em>
                        The primary challenge in tracing the participants in sports and marathon video or images is to
                        detect and localize the jersey/Bib number that may present in different regions of their outfit
                        captured in cluttered environment conditions. In this work, we proposed a new framework based on
                        detecting the human body parts such that both Jersey Bib number and text is localized reliably.
                        To achieve this, the proposed method first detects and localize the human in a given image using
                        Single Shot Multibox Detector (SSD). In the next step, different human body parts namely, Torso,
                        Left Thigh, Right Thigh, that generally contain a Bib number or text region is automatically
                        extracted. These detected individual parts are processed individually to detect the Jersey Bib
                        number/text using a deep CNN network based on the 2-channel architecture based on the novel
                        adaptive weighting loss function. Finally, the detected text is cropped out and fed to a CNN-RNN
                        based deep model abbreviated as CRNN for recognizing jersey/Bib/text. Extensive experiments are
                        carried out on the four different datasets including both bench-marking dataset and a new
                        dataset. The performance of the proposed method is compared with the state-of-the-art methods on
                        all four datasets that indicates the improved performance of the proposed method on all four
                        datasets.
                      </em>
                    </font>
                  </div>
                  <div id="icdar_bib" style="font-family:Courier;display:none;min-width:350px;">
                    <font size="2">
                      <br>
                      @inproceedings{nag2019crnn,<br>
                      title={CRNN Based Jersey-Bib Number/Text Recognition in Sports and Marathon Images},<br>
                      author={Nag, Sauradip and Ramachandra, Raghavendra and Shivakumara, Palaiahnakote and Pal, Umapada
                      and Lu, Tong and Kankanhalli, Mohan},<br>
                      booktitle={2019 International Conference on Document Analysis and Recognition (ICDAR)},<br>
                      pages={1149--1156},<br>
                      year={2019},<br>
                      organization={IEEE}<br>
                      }
                    </font>
                  </div>
                </td>
              </tr>


              <!-- ICASSP 2019 - THUMBNAIL GENERATION -->
              <tr onmouseout="icfhr_stop()" onmouseover="icfhr_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='icfhr_image'><img src='images/icfhr_re_2.png'></div>
                    <img src='images/icfhr_re.png'>
                  </div>
                  <script type="text/javascript">
                    function icfhr_start() {
                      document.getElementById('icfhr_image').style.opacity = "1";
                    }

                    function icfhr_stop() {
                      document.getElementById('icfhr_image').style.opacity = "0";
                    }
                    icfhr_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/abstract/document/8583815/" target="_blank">
                    <papertitle>
                      <font size="3">A New COLD Feature based Handwriting Analysis for Ethnicity/Nationality
                        Identification</font>
                    </papertitle>
                  </a>
                  <br>
                  <font size="3">
                    <strong>
                      <font size="3">Sauradip Nag</font>
                    </strong>,
                    <a href="https://www.surrey.ac.uk/people/ayan-kumar-bhunia" target="_blank">
                      <font size="3">Palaiahnakote Shivakumara</font>
                    </a>,
                    <a href="https://www.surrey.ac.uk/people/ayan-kumar-bhunia" target="_blank">
                      <font size="3">Wu Yirui</font>
                    </a>,
                    <a href="https://www.surrey.ac.uk/people/ayan-kumar-bhunia" target="_blank">
                      <font size="3">Umapada Pal</font>
                    </a>,

                    <a href="https://www.iitr.ac.in/departments/CSE/pages/People+Faculty+Partha_Pratim_Roy.html"
                      target="_blank">
                      <font size="3">Tong Lu</font>
                    </a>
                    <br>
                    <em>International Conference on Frontiers in Handwriting Recognition (ICFHR)</em>, 2018
                    <br>
                    <em>Niagara Falls, USA </em>
                    <strong>
                      <font size="3" style="color:#FF8C00">[ H5-Index : 18 ]</font>
                    </strong>
                    <!-- <br> -->
                    <!-- <font color="red" size="3"><strong>(Oral Presentation)</strong></font> -->
                  </font>
                  <br>
                  <p></p>
                  <a href="javascript:void(0);" onclick="myFunction('icfhr_abs')">
                    <font size="3">Abstract</font>
                  </a> /
                  <!-- <a href="https://github.com/sairajk/Thumbnail-Generation" target="_blank">
                    <font size="3">Code</font>
                  </a> / -->
                  <a href="https://arxiv.org/abs/1806.07072" target="_blank">
                    <font size="3">arXiv</font>
                  </a> /
                  <a href="javascript:void(0);" onclick="myFunction('icfhr_bib')">
                    <font size="3">BibTex</font>
                  </a>
                  <p></p>
                  <div id="icfhr_abs" style="display:none; text-align:justify;min-width:350px;">
                    <font size="3">
                      <em>
                        Identifying crime for forensic investigating teams
                        when crimes involve people of different nationals is challenging.
                        This paper proposes a new method for ethnicity (nationality)
                        identification based on Cloud of Line Distribution (COLD)
                        features of handwriting components. The proposed method, at
                        first, explores tangent angle for the contour pixels in each row
                        and the mean of intensity values of each row in an image for
                        segmenting text lines. For segmented text lines, we use tangent
                        angle and direction of base lines to remove rule lines in the
                        image. We use polygonal approximation for finding dominant
                        points for contours of edge components. Then the proposed
                        method connects the nearest dominant points of every dominant
                        point, which results in line segments of dominant point pairs. For
                        each line segment, the proposed method estimates angle and
                        length, which gives a point in polar domain. For all the line
                        segments, the proposed method generates dense points in polar
                        domain, which results in COLD distribution. As character
                        component shapes change, according to nationals, the shape of
                        the distribution changes. This observation is extracted based on
                        distance from pixels of distribution to Principal Axis of the
                        distribution. Then the features are subjected to an SVM classifier
                        for identifying nationals. Experiments are conducted on a
                        complex dataset, which show the proposed method is effective
                        and outperforms the existing method.
                      </em>
                    </font>
                  </div>
                  <div id="icfhr_bib" style="font-family:Courier;display:none;min-width:350px;">
                    <font size="2">
                      <br>
                      @inproceedings{nag2018new,<br>
                      title={New COLD Feature Based Handwriting Analysis for Enthnicity/Nationality Identification},<br>
                      author={Nag, Sauradip and Shivakumara, Palaiahnakote and Wu, Yirui and Pal, Umapada and Lu,
                      Tong},<br>
                      booktitle={2018 16th International Conference on Frontiers in Handwriting Recognition
                      (ICFHR)},<br>
                      pages={523--527},<br>
                      year={2018},<br>
                      organization={IEEE}<br>
                      }
                    </font>
                  </div>
                </td>
              </tr>


              <!-- WACV 2019 - TEXTURE PAPER -->
              <tr onmouseout="cicba_stop()" onmouseover="cicba_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='cicba_image'><img src='images/cicba_2_re.png'></div>
                    <img src='images/cicba_re.png'>
                  </div>
                  <script type="text/javascript">
                    function cicba_start() {
                      document.getElementById('cicba_image').style.opacity = "1";
                    }

                    function cicba_stop() {
                      document.getElementById('cicba_image').style.opacity = "0";
                    }
                    cicba_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://link.springer.com/chapter/10.1007/978-981-13-2345-4_5" target="_blank">
                    <papertitle>
                      <font size="3">Offline Extraction of Indic Regional Language from Natural Scene Image Using Text
                        Segmentation and Deep Convolutional Sequence</font>
                    </papertitle>
                  </a>
                  <br>
                  <font size="3">
                    <strong>
                      <font size="3">Sauradip Nag</font>
                    </strong>,
                    <a href="https://www.surrey.ac.uk/people/ayan-kumar-bhunia" target="_blank">
                      <font size="3">Pallab Ganguly</font>
                    </a>,
                    <a href="https://www.linkedin.com/in/pranay-mukherjee-701588137/" target="_blank">
                      <font size="3">Sumit Roy</font>
                    </a>,
                    <a href="https://www.linkedin.com/in/pranay-mukherjee-701588137/" target="_blank">
                      <font size="3">Sourab Jha</font>
                    </a>,
                    <a href="https://www.linkedin.com/in/pranay-mukherjee-701588137/" target="_blank">
                      <font size="3"> Krishna Bose</font>
                    </a>,
                    <a href="https://www.linkedin.com/in/pranay-mukherjee-701588137/" target="_blank">
                      <font size="3"> Abhishek Jha</font>
                    </a>,
                    Abhirup Das,
                    <a href="https://www.iitr.ac.in/departments/CSE/pages/People+Faculty+Partha_Pratim_Roy.html"
                      target="_blank">
                      <font size="3"> Kousik Dasgupta</font>
                    </a>
                    <br>
                    <em>International Conference on Computational Intelligence, Communications, and Business Analytics
                      (CICBA)</em>, 2018
                    <br>
                    <em> Kalyani, India <em>
                  </font>
                  <br>
                  <p></p>
                  <a href="javascript:void(0);" onclick="myFunction('cicba_abs')">
                    <font size="3">Abstract</font>
                  </a> /
                  <a href="https://arxiv.org/abs/1806.06208" target="_blank">
                    <font size="3">arXiv</font>
                  </a> /
                  <a href="javascript:void(0);" onclick="myFunction('cicba_bib')">
                    <font size="3">BibTex</font>
                  </a>
                  <p></p>
                  <div id="cicba_abs" style="display:none; text-align:justify;min-width:350px;">
                    <font size="3">
                      <em>
                        Regional language extraction from a natural scene image is always a challenging proposition due
                        to its dependence on the text information extracted from Image. Text Extraction on the other
                        hand varies on different lighting condition, arbitrary orientation, inadequate text information,
                        heavy background influence over text and change of text appearance. This paper presents a novel
                        unified method for tackling the above challenges. The proposed work uses an image correction and
                        segmentation technique on the existing Text Detection Pipeline an Efficient and Accurate Scene
                        Text Detector (EAST). EAST uses standard PVAnet architecture to select features and non maximal
                        suppression to detect text from image. Text recognition is done using combined architecture of
                        MaxOut convolution neural network (CNN) and Bidirectional long short term memory (LSTM) network.
                        After recognizing text using the Deep Learning based approach, the native Languages are
                        translated to English and tokenized using standard Text Tokenizers. The tokens that very likely
                        represent a location is used to find the Global Positioning System (GPS) coordinates of the
                        location and subsequently the regional languages spoken in that location is extracted. The
                        proposed method is tested on a self generated dataset collected from Government of India dataset
                        and experimented on Standard Dataset to evaluate the performance of the proposed technique.
                        Comparative study with a few state-of-the-art methods on text detection, recognition and
                        extraction of regional language from images shows that the proposed method outperforms the
                        existing methods.
                      </em>
                    </font>
                  </div>
                  <div id="cicba_bib" style="font-family:Courier;display:none;min-width:350px;">
                    <font size="2">
                      <br>
                      @incollection{nag2018offline,<br>
                      title={Offline Extraction of Indic Regional Language from Natural Scene Image Using Text
                      Segmentation and Deep Convolutional Sequence},<br>
                      author={Nag, Sauradip and Ganguly, Pallab Kumar and Roy, Sumit and Jha, Sourab and Bose, Krishna
                      and Jha, Abhishek and Dasgupta, Kousik},<br>
                      booktitle={Methodologies and Application Issues of Contemporary Computing Framework},<br>
                      pages={49--68},<br>
                      year={2018},<br>
                      publisher={Springer}<br>
                      }
                    </font>
                  </div>
                </td>
              </tr>


              <!-- <tr onmouseout="loss_stop()" onmouseover="loss_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='loss_image'><img src='images/loss_after.png'></div>
                <img src='images/loss_before.png'>
              </div>
              <script type="text/javascript">
                function loss_start() {
                  document.getElementById('loss_image').style.opacity = "1";
                }

                function loss_stop() {
                  document.getElementById('loss_image').style.opacity = "0";
                }
                loss_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=1xpZ0fL9h1y9RfcTyPgVkxUrF3VwdkBvq">
                <papertitle>A General and Adaptive Robust Loss Function</papertitle>
              </a>
              <br>
              <strong>Jonathan T. Barron</strong>
              <br>
              <em>CVPR</em>, 2019 &nbsp <font color="red"><strong>(Oral Presentation, Best Paper Award Finalist)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/1701.03077">arxiv</a> /
              <a href="https://drive.google.com/open?id=1HNveL7xSNh6Ss7sxLK8Mw2L1Fc-rRhL4">supplement</a> /
              <a href="https://youtu.be/BmNKbnF69eY">video</a> /
              <a href="https://www.youtube.com/watch?v=4IInDT_S0ow&t=37m22s">talk</a> /
              <a href="https://drive.google.com/file/d/1GzRYRIfLHvNLT_QwjHoBjHkBbs3Nbf0x/view?usp=sharing">slides</a> /
              <a href="https://github.com/google-research/google-research/tree/master/robust_loss">tensorflow code</a> /
              <a href="https://github.com/jonbarron/robust_loss_pytorch">pytorch code</a> /
              <a href="data/BarronCVPR2019_reviews.txt">reviews</a> /
              <a href="data/BarronCVPR2019.bib">bibtex</a>
              <p></p>
              <p>A single robust loss function is a superset of many other common robust loss functions, and allows training to automatically adapt the robustness of its own loss.</p>
            </td>
          </tr> -->

            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <!-- <td style="padding:20px;padding-top:8px;padding-bottom:5px;width:100%;vertical-align:middle">
                  <heading>
                    <font size="4">Projects</font>
                  </heading>
                </td> -->
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>
                    <font size="5">Projects</font>
                  </heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr onmouseout="proj_sisr_stop()" onmouseover="proj_sisr_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='proj_sisr_image'><img src='images/robot_loco_short.gif'></div>
                    <img src='images/robot_loco_short.gif'>
                  </div>
                  <script type="text/javascript">
                    function proj_sisr_start() {
                      document.getElementById('proj_sisr_image').style.opacity = "1";
                    }

                    function proj_sisr_stop() {
                      document.getElementById('proj_sisr_image').style.opacity = "0";
                    }
                    proj_sisr_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle;text-align:justify">
                  <a href="https://github.com/sauradip/vision_based_robot_navigation" target="_blank">
                    <papertitle>
                      <font size="3">Computer Vision based Robot Locomotion</font>
                    </papertitle>
                  </a>
                  <br><br>
                  <font size="3">
                    <!-- Image super resolution is the process of increasing the resolution of a Low Resolution (LR) image by generating pixels which interpolate best between the given LR image and the required High Resolution (HR) image. The code was written using Keras (TF backend) in Python3 and comes with an easy to use interface. -->
                    <!-- The task has numerous applications like in Satellite Imaging, Surveillance, Medical image processing, Biometrics, etc. -->
                    Vision-based robot navigation has long been a fundamental goal in both robotics and computer vision
                    research. However, we do not require all semantic labelling of a particular environment for a robot
                    to move.
                    It requires only the floor part of a environment to navigate its path as we are dealing with ground
                    based robots. Depth information is particularly useful in predicting how much the robot can move in
                    a particular direction. Hence, the marriage between both the vision tasks provides us a free space
                    map which enables the robot to move freely in a given direction.Hence, we have designed a novel
                    motion control algorithm
                    which enables the robot to naviagte through obstacles in its path. This is implemented in Pytorch.
                  </font>
                  <br>
                  <p></p>
                  <a href="https://github.com/sauradip/vision_based_robot_navigation" target="_blank">
                    <font size="3">Code</font>
                  </a>
                  <!-- <a href="https://github.com/YapengTian/Single-Image-Super-Resolution" target="_blank">
                    <font size="3">Papers on Super Resolution</font>
                  </a> -->
                  <p></p>
                </td>

              </tr>

              <tr onmouseout="proj_sisr_stop()" onmouseover="proj_sisr_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='proj_sisr_image'><img src='images/robot.gif'></div>
                    <img src='images/robot.gif'>
                  </div>
                  <script type="text/javascript">
                    function proj_sisr_start() {
                      document.getElementById('proj_sisr_image').style.opacity = "1";
                    }

                    function proj_sisr_stop() {
                      document.getElementById('proj_sisr_image').style.opacity = "0";
                    }
                    proj_sisr_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle;text-align:justify">
                  <a href="https://github.com/sauradip/autonomous_robot_locomotion" target="_blank">
                    <papertitle>
                      <font size="3">Autonomous Robot Locomotion in prespecified path</font>
                    </papertitle>
                  </a>
                  <br><br>
                  <font size="3">
                    <!-- Image super resolution is the process of increasing the resolution of a Low Resolution (LR) image by generating pixels which interpolate best between the given LR image and the required High Resolution (HR) image. The code was written using Keras (TF backend) in Python3 and comes with an easy to use interface. -->
                    <!-- The task has numerous applications like in Satellite Imaging, Surveillance, Medical image processing, Biometrics, etc. -->
                    Robotics have helped humans greatly in achieving everyday tasks. Robots are designed to work in any
                    environment and perform task on behalf of humans. They operate under real-world and real-time
                    constraints where sensors and effectors with specific physical characteristics have to be
                    controlled. In many cases, those robots are controlled manually to move from one destination to
                    another. An Unmanned Ground Vehicle (UGV) is a vehicle that operates while in contact with the
                    ground and without an onboard human presence. We have used
                    one of such robots to demonstrate the custom path which user can choose. Currently we have
                    implemented 2 such custom paths. This has been implemented in Python and ROS.
                  </font>
                  <br>
                  <p></p>
                  <a href="https://github.com/sauradip/autonomous_robot_locomotion" target="_blank">
                    <font size="3">Code</font>
                  </a>
                  <!-- <a href="https://github.com/YapengTian/Single-Image-Super-Resolution" target="_blank">
                    <font size="3">Papers on Super Resolution</font>
                  </a> -->
                  <p></p>
                </td>

              </tr>

              <tr onmouseout="proj_sisr_stop()" onmouseover="proj_sisr_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='proj_sisr_image'><img src='images/proj_3.gif'></div>
                    <img src='images/robot.gif'>
                  </div>
                  <script type="text/javascript">
                    function proj_sisr_start() {
                      document.getElementById('proj_sisr_image').style.opacity = "1";
                    }

                    function proj_sisr_stop() {
                      document.getElementById('proj_sisr_image').style.opacity = "0";
                    }
                    proj_sisr_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle;text-align:justify">
                  <a href="https://github.com/sauradip/Boundary-Growing-in-Python" target="_blank">
                    <papertitle>
                      <font size="3">Boundary Growing Algorithm for Recovery of Torso from Corrupt Face</font>
                    </papertitle>
                  </a>
                  <br><br>
                  <font size="3">
                    <!-- Image super resolution is the process of increasing the resolution of a Low Resolution (LR) image by generating pixels which interpolate best between the given LR image and the required High Resolution (HR) image. The code was written using Keras (TF backend) in Python3 and comes with an easy to use interface. -->
                    <!-- The task has numerous applications like in Satellite Imaging, Surveillance, Medical image processing, Biometrics, etc. -->
                    Automatic face detection has been intensively studied for human related recognition systems.
                    However, there has been a very little work in recovering face from a corrupted face image. We have
                    designed a boundary growing algorithm where we incrementally grew the boundary of the corrupted face
                    image and passed into HAAR cascade classifier to get confidence score. We kept on doing until we
                    reach the maximum confidence. After that we used tailor measurements to recover the torso part of
                    the human. This has been implemented using OpenCV and Python.

                  </font>
                  <br>
                  <p></p>
                  <a href="https://github.com/sauradip/Boundary-Growing-in-Python" target="_blank">
                    <font size="3">Code</font>
                  </a>
                  <!-- <a href="https://github.com/YapengTian/Single-Image-Super-Resolution" target="_blank">
                    <font size="3">Papers on Super Resolution</font>
                  </a> -->
                  <p></p>
                </td>

              </tr>


              <tr onmouseout="proj_sisr_stop()" onmouseover="proj_sisr_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='proj_sisr_image'><img src='images/proj_4.gif'></div>
                    <img src='images/proj_4.gif'>
                  </div>
                  <script type="text/javascript">
                    function proj_sisr_start() {
                      document.getElementById('proj_sisr_image').style.opacity = "1";
                    }

                    function proj_sisr_stop() {
                      document.getElementById('proj_sisr_image').style.opacity = "0";
                    }
                    proj_sisr_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle;text-align:justify">
                  <a href="https://github.com/sauradip/BaseLine-Remover" target="_blank">
                    <papertitle>
                      <font size="3">BaseLine Remover from Doument Images</font>
                    </papertitle>
                  </a>
                  <br><br>
                  <font size="3">
                    <!-- Image super resolution is the process of increasing the resolution of a Low Resolution (LR) image by generating pixels which interpolate best between the given LR image and the required High Resolution (HR) image. The code was written using Keras (TF backend) in Python3 and comes with an easy to use interface. -->
                    <!-- The task has numerous applications like in Satellite Imaging, Surveillance, Medical image processing, Biometrics, etc. -->
                    This a small Matlab Implementation for Removal of Base Line from document using Edge Directional
                    Kernel stated in paper " Edge enhancement algorithm for low-dose X-ray fluoroscopic imaging " by Lee
                    et al. Here Baselines are Removed using Edge Directional Kernel . BaseLine Removal is an important
                    topic in Document Image Analysis . In this paper Lee et al proposed removal of Noise from XRAY
                    Images using Edge Directional Kernel and High Pass Filter but since our Noise is only Baseline we
                    used a clever trick to implement only the Edge Directional Kernel and the reuslts are quite neat.
                    This method works well for Half Page Document and Cropped Line Images , however if full page images
                    are preprocessed then it may work pretty well. 

                  </font>
                  <br>
                  <p></p>
                  <a href="https://github.com/sauradip/BaseLine-Remover" target="_blank">
                    <font size="3">Code</font>
                  </a> / 
                  <a href="https://github.com/YapengTian/Single-Image-Super-Resolution" target="_blank">
                    <font size="3">Paper</font>
                  </a>
                  <p></p>
                </td>

              </tr>
            </tbody>
          </table>




          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>
                    <font size="5">Collaborators</font>
                  </heading>
                </td>
              </tr>
              <tr>
                <td style="padding:0px;width:75%;vertical-align:middle">
                  <!-- Projects -->

                  <div class="wrapper">

                    <img src="images/Umapada_Pal.jpg" class="image--cover">

                    <img src="images/shivakumara.jpeg" alt="" class="image--cover" />

                    <img src="images/raghavendra.jpg" alt="" class="image--cover" />

                    <img src="images/Michael-Blumenstein.jpg" alt="" class="image--cover" />
                  </div>
              </tr>
              <tr>
                <td style="padding:0px;width:75%;vertical-align:middle">
                  <!-- Projects -->
                  <!-- <div class="wrapper">

        <span style="padding-left:30px;padding-right:30px;">Umapada Pal</span>
        <span style="padding-left:30px;padding-right:30px;"></span>
        <span style="padding-left:30px;padding-right:30px;">Palaiahnakote Shivakumara</span>
        <span style="padding-left:30px;padding-right:30px;"></span>
        <span style="padding-left:30px;padding-right:30px;">Raghavendra Ramachandra</span>
        <span style="padding-left:30px;padding-right:30px;"></span>
        <span style="padding-left:30px;padding-right:30px;">Michael Blumenstein</span>

      </div> -->
                  <div class="demo" style="padding:5px;">
                    <span class="collab"><a style="font-size:16px"
                        href="https://scholar.google.co.in/citations?user=2_z_CogAAAAJ&hl=en">Umapada Pal</a></span>
                    <span class="collab" style="padding-left:35px"><a style="font-size:16px"
                        href="https://scholar.google.co.in/citations?user=XfpbOc4AAAAJ&hl=en">P Shivakumara</a></span>
                    <span class="collab"><a style="font-size:16px"
                        href="https://scholar.google.co.in/citations?user=OIYIrmIAAAAJ&hl=en">R Ramachandra</a></span>
                    <span class="collab"><a style="font-size:16px"
                        href="https://scholar.google.co.in/citations?user=4m2G-H8AAAAJ&hl=en">Michael
                        Blumenstein</a></span>
                  </div>
                  <div class="wrapper">

                    <img src="images/TongLu.jpg" class="image--cover">

                    <img src="images/parthapratim.jpg" alt="" class="image--cover" />

                    <img src="images/mohan_kankanhalli.jpg" alt="" class="image--cover" />

                    <img src="images/kd_sir_re.jpg" alt="" class="image--cover" />

                  </div>
                  <!-- <div class="wrapper">

        <span style="padding-left:30px;padding-right:30px;">Umapada_Pal</span>
        <span style="padding-left:30px;padding-right:30px;"></span>
        <span style="padding-left:30px;padding-right:30px;">Umapada_Pal</span>
        <span style="padding-left:30px;padding-right:30px;"></span>
        <span style="padding-left:30px;padding-right:30px;">Umapada_Pal</span>
        <span style="padding-left:30px;padding-right:30px;"></span>
        <span style="padding-left:30px;padding-right:30px;">Umapada_Pal</span>

      </div> -->
                  <div class="demo">
                    <span class="collab" style="padding-left:30px"><a style="font-size:16px"
                        href="https://cs.nju.edu.cn/_upload/tpl/00/cd/205/template205/Publications.html">Lu
                        Tong</a></span>
                    <span class="collab" style="padding-left:32px"><a style="font-size:16px"
                        href="https://scholar.google.co.in/citations?user=moDpyKkAAAAJ&hl=en">Partha Roy</a></span>
                    <span class="collab" style="padding-left:-10px;"><a style="font-size:16px"
                        href="https://scholar.google.com/citations?user=6Lx_eowAAAAJ&hl=th">M Kankanhalli</a></span>
                    <span class="collab"><a style="font-size:16px"
                        href="https://scholar.google.co.in/citations?user=63DEdC0AAAAJ&hl=en">Kousik Dasgupta</a></span>
                  </div>
              </tr>
              <!--  -->
            </tbody>
          </table>
          <!-- Footer - Template Credits -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:center;font-size:medium;">
                    Visitor Map
                    <!--                     <a href="https://jonbarron.info/" target="_blank">Dr. Jon Barron</a> -->
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <center><a href="https://livetrafficfeed.com/live-maps-visitor" data-size="65" data-type="12" data-root="1"
              id="LTF_mapjs_website">Maps Visitor</a>
            <script type="text/javascript" src="//cdn.livetrafficfeed.com/static/mapjs/live.v2.js"></script><noscript><a
                href="http://livetrafficfeed.com/live-maps-visitor">Maps Visitor</a></noscript></center>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:center;font-size:small;">
                    Template credits :
                    <a href="https://jonbarron.info/" target="_blank">Dr. Jon Barron</a>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>


        </td>
      </tr>
  </table>
</body>

</html>
